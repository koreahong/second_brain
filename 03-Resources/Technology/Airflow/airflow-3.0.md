---
title: airflow 3.0
type: resource
tags:
- pipeline
- datahub
- dbt
- airflow
- technology
- python
- aws
created: '2025-11-30'
updated: '2025-11-30'
aliases: []
---

## Asset ê°œë…

### 1. Asset ì—…ë°ì´íŠ¸ ê¸°ì¤€

- Task ì‹¤í–‰ ì„±ê³µ + outlets=[Asset(...)] ì„ ì–¸
  â†’ í•´ë‹¹ Assetì´ ìƒˆë¡œ ê°±ì‹ ëë‹¤ê³  Airflowê°€ ê¸°ë¡

- ì¦‰, â€œíŒŒì¼ì´ ì‹¤ì œë¡œ ìˆ˜ì •ë˜ì—ˆëŠ”ì§€â€ë¥¼ ê°ì§€í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼,
  â€œì´ Taskê°€ ì„±ê³µì ìœ¼ë¡œ ëŒì•˜ìœ¼ë‹ˆ ì´ Assetì€ ìµœì‹  ë²„ì „ì´ë‹¤â€ ë¼ê³  ë³´ëŠ” ê²ƒ

ì˜ˆì‹œ:

```python
dag1_asset = Asset("s3://dag1/output_1.txt")

with DAG(...):
    BashOperator(
        task_id="producing_task_1",
        bash_command="echo hello > /tmp/output.txt && aws s3 cp /tmp/output.txt s3://dag1/output_1.txt",
        outlets=[dag1_asset],
    )

```

â¡ï¸ ìœ„ Taskê°€ ì„±ê³µí•˜ë©´, Airflow DB(Event Log)ì—

s3://dag1/output_1.txt Assetì´ â€œ2025-09-16 12:00 UTCì— ê°±ì‹ ë¨â€ ì´ë¼ê³  ê¸°ë¡ë¨.

---

### 2. Inletì´ ë°˜ì‘í•˜ëŠ” ê¸°ì¤€

ì†Œë¹„ì ìª½(inlets or schedule=[Asset(...)])ì€ Airflowê°€ ê¸°ë¡í•œ Asset ê°±ì‹  ì´ë²¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.

ì˜ˆì‹œ:

```python
with DAG(
    dag_id="asset_consumes_1",
    schedule=[dag1_asset],  # <-- ì´ Assetì´ ì—…ë°ì´íŠ¸ë˜ë©´ DAG ì‹¤í–‰
    ...
):
    BashOperator(
        task_id="consuming_1",
        bash_command="echo consuming asset1",
    )

```

â¡ï¸ ë™ì‘ íë¦„:

1. producing_task_1 ì‹¤í–‰ ì„±ê³µ â†’ Asset Event ê¸°ë¡ë¨
1. Airflow Schedulerê°€ dag1_asset ê°±ì‹  ì´ë²¤íŠ¸ ê°ì§€
1. asset_consumes_1 DAG ìë™ Trigger
---

### 3. ë¬¼ë¦¬ì  íŒŒì¼ ë³€ê²½ vs Airflow ì´ë²¤íŠ¸

- AirflowëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **íŒŒì¼ ë³€ê²½ ê°ì‹œ(File Watcher)**ë¥¼ í•˜ì§€ ì•ŠìŒ
- ëŒ€ì‹  ìê¸° DAG ì‹¤í–‰ ê¸°ë¡ì„ ì‹ ë¢°í•¨
- ë§Œì•½ ì‹¤ì œ íŒŒì¼ ë³€ê²½ ì—¬ë¶€ë¥¼ ë°˜ì˜í•˜ê³  ì‹¶ë‹¤ë©´ â†’ Sensor Operatorë¥¼ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ í•¨
ì˜ˆì‹œ (S3KeySensor ì¡°í•©):

```python
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor

s3_sensor = S3KeySensor(
    task_id="check_s3_file",
    bucket_key="dag1/output_1.txt",
    bucket_name="dag1",
    poke_interval=60,
    timeout=600,
)

```

---

### 4. ì •ë¦¬

- outlets=[Asset(...)] â†’ Task ì„±ê³µ = Asset ê°±ì‹  ì´ë²¤íŠ¸ ê¸°ë¡
- inlets / schedule=[Asset(...)] â†’ ê·¸ ì´ë²¤íŠ¸ë¥¼ ë³´ê³  DAG/Task ì‹¤í–‰
- ì‹¤ì œ íŒŒì¼ ì—…ë°ì´íŠ¸ ê²€ì¦ê¹Œì§€ í•˜ê³  ì‹¶ë‹¤ë©´ â†’ Sensorë¥¼ ì¶”ê°€í•´ì„œ ë³´ê°•í•´ì•¼ í•¨
## ì½”ë“œ ìœ ë‹›

- airflow 2.0ì—ì„œ íŒŒì¼ë‹¹ í•œê°œì˜ dagë§Œì„ ì •ì˜í•´ì„œ íŒŒì¼ = dagì˜€ë˜ ìœ ë‹›ê³¼ ë‹¤ë¥´ê²Œ, airflow 3.0ì€ í•œ íŒŒì¼ì—ì„œ í•œ íŒŒì´í”„ë¼ì¸ ì „ë¶€ë¥¼ ì •ì˜í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì§„í–‰ë¨.
- í•œ íŒŒì¼ì—ì„œ í•œ íŒŒì´í”„ë¼ì¸ì„ ì‘ì„±í•˜ë©´ ê·¸ ê²ƒì— ëŒ€í•œ ì •ë³´ë“¤ì´ ë‚¯ê°œë¡œ ì˜¬ë¼ì˜´. â‡’ ì½”ë“œê°€ ê°™ê²Œ ë³´ì„, ì´ëŸ° ë¶€ë¶„ì´ ì‚´ì§ ë¶ˆí¸í• ë“¯
<details>
<summary>ì½”ë“œ ì˜ˆì‹œ</summary>

```python
from __future__ import annotations

# [START asset_def]
import pendulum

from airflow.providers.standard.operators.bash import BashOperator
from airflow.sdk import DAG, Asset
from airflow.timetables.assets import AssetOrTimeSchedule
from airflow.timetables.trigger import CronTriggerTimetable

dag1_asset = Asset("s3://dag1/output_4.txt", extra={"hi": "bye"})
dag2_asset = Asset("s3://dag2/output_4.txt", extra={"hi": "bye"})
dag3_asset = Asset("s3://dag3/output_5.txt", extra={"hi": "bye"})

with DAG(
    dag_id="asset_produces_3",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule="@daily",
    tags=["produces", "asset-scheduled"],
) as dag1:
    # [START task_outlet]
    BashOperator(outlets=[dag1_asset], task_id="producing_task_3", bash_command="sleep 5")
    # [END task_outlet]

with DAG(
    dag_id="asset_produces_4",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule=None,
    tags=["produces", "asset-scheduled"],
) as dag2:
    BashOperator(outlets=[dag2_asset], task_id="producing_task_4", bash_command="sleep 5")

# [START dag_dep]
with DAG(
    dag_id="asset_consumes_3",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule=[dag1_asset],
    tags=["consumes", "asset-scheduled"],
) as dag3:
    # [END dag_dep]
    BashOperator(
        outlets=[Asset("s3://consuming_3_task/asset_other.txt")],
        task_id="consuming_3",
        bash_command="sleep 5",
    )

with DAG(
    dag_id="asset_consumes_3_and_4",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule=[dag1_asset, dag2_asset],
    tags=["consumes", "asset-scheduled"],
) as dag4:
    BashOperator(
        outlets=[Asset("s3://consuming_4_task/asset_other_unknown.txt")],
        task_id="consuming_4",
        bash_command="sleep 5",
    )

with DAG(
    dag_id="asset_consumes_3_never_scheduled",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule=[
        dag1_asset,
        Asset("s3://unrelated/this-asset-doesnt-get-triggered"),
    ],
    tags=["consumes", "asset-scheduled"],
) as dag5:
    BashOperator(
        outlets=[Asset("s3://consuming_4_task/asset_other_unknown.txt")],
        task_id="consuming_3",
        bash_command="sleep 5",
    )

with DAG(
    dag_id="asset_consumes_unknown_never_scheduled",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule=[
        Asset("s3://unrelated/asset3.txt"),
        Asset("s3://unrelated/asset_other_unknown.txt"),
    ],
    tags=["asset-scheduled"],
) as dag6:
    BashOperator(
        task_id="unrelated_task",
        outlets=[Asset("s3://unrelated_task/asset_other_unknown.txt")],
        bash_command="sleep 5",
    )

with DAG(
    dag_id="consume_3_and_4_with_asset_expressions",
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule=(dag1_asset & dag2_asset),
) as dag5:
    BashOperator(
        outlets=[Asset("s3://consuming_4_task/asset_other_unknown.txt")],
        task_id="consume_3_and_4_with_asset_expressions",
        bash_command="sleep 5",
    )
with DAG(
    dag_id="consume_3_or_4_with_asset_expressions",
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule=(dag1_asset | dag2_asset),
) as dag6:
    BashOperator(
        outlets=[Asset("s3://consuming_4_task/asset_other_unknown.txt")],
        task_id="consume_3_or_4_with_asset_expressions",
        bash_command="sleep 5",
    )
with DAG(
    dag_id="consume_3_or_both_4_and_3_with_asset_expressions",
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule=(dag1_asset | (dag2_asset & dag3_asset)),
) as dag7:
    BashOperator(
        outlets=[Asset("s3://consuming_4_task/asset_other_unknown.txt")],
        task_id="consume_3_or_both_4_and_3_with_asset_expressions",
        bash_command="sleep 5",
    )
with DAG(
    dag_id="conditional_asset_and_time_based_timetable_1",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    schedule=AssetOrTimeSchedule(
        timetable=CronTriggerTimetable("0 1 * * 3", timezone="UTC"), assets=(dag1_asset & dag2_asset)
    ),
    tags=["asset-time-based-timetable"],
) as dag8:
    BashOperator(
        outlets=[Asset("s3://asset_time_based/asset_other_unknown.txt")],
        task_id="conditional_asset_and_time_based_timetable_1",
        bash_command="sleep 5",
    )
# [END asset_def]

```

</details>

## dbt ì‘ì„±

## datahub ì—°ê²°

---

## ğŸ“ Related

<!-- ìë™ ìƒì„±ëœ ì„¹ì…˜ - ìˆ˜ë™ìœ¼ë¡œ ë§í¬ë¥¼ ì¶”ê°€í•˜ì„¸ìš” -->

### Projects

### Knowledge

### Insights

