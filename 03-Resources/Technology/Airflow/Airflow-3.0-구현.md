---
created: '2025-11-30'
updated: '2025-11-30'
tags:
  - Technology
  - Airflow
  - DataHub
  - Metadata
  - DataPlatform
related:
  - Keycloak-OIDC-ì¸ì¦
  - DBT-Incremental-êµ¬í˜„
  - Airflow-3.0-ì—…ê·¸ë ˆì´ë“œ-ë°°ê²½
---
# Airflow 3.0 êµ¬í˜„

## ê°œìš”

Apache Airflow 3.0ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ë©´ì„œ ì£¼ìš” ì•„í‚¤í…ì²˜ ë³€ê²½ ì‚¬í•­ì„ ì ìš©í•˜ê³ , DataHub ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ì»¤ìŠ¤í…€ REST API Connectorë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤.

**í•µì‹¬ êµ¬í˜„:**
- Custom REST API Connector (DataHub ì—°ë™)
- Asset-based lineage ì‹œìŠ¤í…œ
- Serialized DAG êµ¬ì¡° íŒŒì‹±
- Metadata DB ì§ì ‘ ì¿¼ë¦¬
- Keycloak OIDC ì¸ì¦ í†µí•©

## ì£¼ìš” ê¸°ìˆ  ìŠ¤íƒ

- **Airflow 3.1.3** (3.0.6 â†’ 3.1.3 ì—…ê·¸ë ˆì´ë“œ)
- **PostgreSQL** (Metadata DB)
- **DataHub** (ë©”íƒ€ë°ì´í„° ì¹´íƒˆë¡œê·¸)
- **Keycloak OIDC** (ì¸ì¦)
- **Python 3.10+**
- **psycopg2** (DB ì§ì ‘ ì¿¼ë¦¬)

## 1. Custom REST API Connector êµ¬ì¡°

### ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DataHub Ingestion                       â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          AirflowSource (airflow_source.py)            â”‚  â”‚
â”‚  â”‚  - DAG/Task ë©”íƒ€ë°ì´í„° ì¶”ì¶œ                            â”‚  â”‚
â”‚  â”‚  - Asset lineage ì¶”ì¶œ                                  â”‚  â”‚
â”‚  â”‚  - Task dependencies ì¶”ì¶œ                              â”‚  â”‚
â”‚  â”‚  - DataHub work units ìƒì„±                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                               â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ AirflowAPIClient        â”‚  â”‚ AirflowMetadataDBClient   â”‚  â”‚
â”‚  â”‚ (api_client.py)         â”‚  â”‚ (metadata_db_client.py)   â”‚  â”‚
â”‚  â”‚ - REST API v2 í˜¸ì¶œ      â”‚  â”‚ - serialized_dag íŒŒì‹±     â”‚  â”‚
â”‚  â”‚ - Pagination ì²˜ë¦¬       â”‚  â”‚ - Task dependencies ì¶”ì¶œ  â”‚  â”‚
â”‚  â”‚ - Filtering ì ìš©        â”‚  â”‚ - Asset outlets ì¶”ì¶œ      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                               â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚                       â”‚
â”‚  â”‚ AirflowAuthClient       â”‚            â”‚                       â”‚
â”‚  â”‚ (auth.py)               â”‚            â”‚                       â”‚
â”‚  â”‚ - Keycloak OIDC token   â”‚            â”‚                       â”‚
â”‚  â”‚ - Basic Auth fallback   â”‚            â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                               â”‚
          â”‚ HTTP                          â”‚ PostgreSQL
          â–¼                               â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Airflow REST API   â”‚     â”‚ Airflow Metadata DB  â”‚
  â”‚ (Keycloak ì¸ì¦)    â”‚     â”‚ (PostgreSQL)         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### íŒŒì¼ êµ¬ì¡°

```
infrastructure/datahub/custom_sources/airflow/
â”œâ”€â”€ __init__.py                  # Package ì´ˆê¸°í™”
â”œâ”€â”€ config.py                    # Configuration í´ë˜ìŠ¤
â”œâ”€â”€ auth.py                      # Keycloak OIDC ì¸ì¦ í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ api_client.py                # Airflow API v2 í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ metadata_db_client.py        # Airflow metadata DB ì§ì ‘ ì¿¼ë¦¬
â”œâ”€â”€ metadata_utils.py            # ë©”íƒ€ë°ì´í„° ë³€í™˜ ìœ í‹¸ë¦¬í‹°
â””â”€â”€ airflow_source.py            # ë©”ì¸ Source í´ë˜ìŠ¤
```

## 2. í•µì‹¬ ê¸°ìˆ  êµ¬í˜„

### 2.1 Serialized DAG íŒŒì‹± (Airflow 3.x êµ¬ì¡° ë³€ê²½ ëŒ€ì‘)

**ë¬¸ì œ:** Airflow 3.xì—ì„œ `serialized_dag` í…Œì´ë¸”ì˜ JSON êµ¬ì¡°ê°€ ë³€ê²½ë˜ì–´ ê¸°ì¡´ DataHub í”ŒëŸ¬ê·¸ì¸ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ.

**Airflow 2.x êµ¬ì¡°:**
```json
{
  "tasks": [
    {
      "task_id": "task1",
      "downstream_task_ids": ["task2"]
    }
  ]
}
```

**Airflow 3.x êµ¬ì¡°:**
```json
{
  "dag": {
    "tasks": [
      {
        "__var": {
          "task_id": "task1",
          "downstream_task_ids": ["task2"]
        },
        "__type": "operator"
      }
    ]
  }
}
```

**êµ¬í˜„ ì½”ë“œ:**

```python
# metadata_db_client.py:129-166
def get_task_dependencies(self, dag_id: str) -> Dict[str, List[str]]:
    """
    Extract task dependencies from serialized DAG data
    """
    with self._get_connection() as conn:
        with conn.cursor() as cursor:
            # Query serialized DAG data (Airflow 3.x)
            query = """
                SELECT data
                FROM serialized_dag
                WHERE dag_id = %s
            """
            cursor.execute(query, (dag_id,))
            result = cursor.fetchone()
            
            dag_data = result["data"]
            if isinstance(dag_data, str):
                dag_data = json.loads(dag_data)
            
            # Airflow 3.x: tasks in "dag" wrapper with __var/__type
            if "dag" in dag_data:
                tasks = dag_data["dag"].get("tasks", [])
            else:
                tasks = dag_data.get("tasks", [])
            
            upstream_deps = {}
            downstream_info = {}
            
            for task in tasks:
                # Airflow 3.x uses __var to store actual task data
                if "__var" not in task:
                    continue
                
                task_var = task["__var"]
                task_id = task_var.get("task_id")
                
                # Initialize upstream deps
                upstream_deps[task_id] = []
                
                # Collect downstream info (will be inverted)
                downstream_task_ids = task_var.get("downstream_task_ids", [])
                downstream_info[task_id] = downstream_task_ids
            
            # Second pass: Invert downstream â†’ upstream
            for task_id, downstream_ids in downstream_info.items():
                for downstream_id in downstream_ids:
                    if downstream_id in upstream_deps:
                        upstream_deps[downstream_id].append(task_id)
            
            return upstream_deps
```

**í•µì‹¬:**
1. `__var` í‚¤ë¡œ ì‹¤ì œ task ë°ì´í„° ì¶”ì¶œ
2. `downstream_task_ids`ë¥¼ ì—­ìˆœìœ¼ë¡œ ë³€í™˜í•˜ì—¬ `upstream_deps` ìƒì„±
3. DataHubê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ (task â†’ upstream list) ì œê³µ

### 2.2 Asset Outlet ì¶”ì¶œ

**ë¬¸ì œ:** Airflow 3.xì˜ Asset (Dataset) outlet ì •ë³´ê°€ REST APIì— ì—†ìŒ.

**í•´ê²°:** Metadata DB ì§ì ‘ ì¿¼ë¦¬ (`task_outlet_asset_reference` í…Œì´ë¸”)

```python
# metadata_db_client.py:195-222
def get_task_outlets(self, dag_id: str) -> Dict[str, List[str]]:
    """
    Extract task outlets (Dataset outputs) from metadata DB
    """
    with self._get_connection() as conn:
        with conn.cursor() as cursor:
            query = """
                SELECT 
                    sd.dag_id,
                    tor.task_id,
                    a.uri
                FROM task_outlet_asset_reference tor
                JOIN asset a ON tor.asset_id = a.id
                JOIN serialized_dag sd ON tor.dag_id = sd.dag_id
                WHERE sd.dag_id = %s
            """
            cursor.execute(query, (dag_id,))
            results = cursor.fetchall()
            
            # Group outlets by task_id
            outlets = {}
            for row in results:
                task_id = row["task_id"]
                uri = row["uri"]
                
                if task_id not in outlets:
                    outlets[task_id] = []
                outlets[task_id].append(uri)
            
            return outlets
```

### 2.3 Cosmos DBT Outlet ì¶”ë¡ 

**ë¬¸ì œ:** Airflow Cosmosë¡œ ì‹¤í–‰ë˜ëŠ” DBT taskëŠ” ëª…ì‹œì  outletì´ ì—†ìŒ.

**í•´ê²°:** Task ì´ë¦„ íŒ¨í„´ìœ¼ë¡œ DBT ëª¨ë¸ URN ì¶”ë¡ 

```python
# airflow_source.py:509-578
def _infer_cosmos_outlet(self, task_id: str) -> Optional[str]:
    """
    Infer dataset outlet URN from Cosmos DBT task name
    
    Cosmos task naming pattern: {group_id}.{model_name}.{run|test}
    Example: transform_data.stg_us_sec_meta_base.run
    """
    import re
    
    group_pattern = self.config.cosmos_task_group_pattern
    
    # Match Cosmos task pattern: {group}.{model}.run or {group}.{model}_run
    cosmos_pattern = rf"^{re.escape(group_pattern)}\.(.+?)(?:\.run|_run)$"
    match = re.match(cosmos_pattern, task_id)
    
    if not match:
        return None
    
    model_name = match.group(1)
    
    # Determine database.schema from mapping
    db_schema = None
    if self.config.cosmos_schema_mapping:
        # Check model prefix (sorted by longest first)
        for prefix, mapping in sorted(
            self.config.cosmos_schema_mapping.items(),
            key=lambda x: len(x[0]),
            reverse=True,
        ):
            if model_name.startswith(prefix):
                db_schema = mapping
                break
    
    # Fall back to default database.schema
    if not db_schema:
        db_schema = f"{self.config.cosmos_outlet_database}.{self.config.cosmos_outlet_schema}"
    
    # Build the dataset URN
    dataset_name = f"{db_schema}.{model_name}"
    
    # Platform: DBT (taskâ†’modelâ†’table) or direct to Snowflake
    platform = (
        "dbt"
        if self.config.cosmos_outlet_to_dbt
        else self.config.cosmos_outlet_platform
    )
    
    dataset_urn = make_dataset_urn(
        platform=platform, name=dataset_name, env=self.config.env
    )
    
    logger.info(
        f"Inferred Cosmos outlet: task '{task_id}' â†’ {platform}:'{dataset_name}'"
    )
    return dataset_urn
```

**Schema Mapping ì˜ˆì‹œ:**
```yaml
cosmos_schema_mapping:
  # Core dimension tables
  "dim_holiday": "qraft_origin.core"
  "dim_ticker": "qraft_origin.core"
  # Employee dimension tables
  "dim_flex": "qraft_automation.employee"
  "flex_": "qraft_automation.employee"
  # Layer-based mapping
  "int_": "qraft_origin.intermediate"
  "stg_": "qraft_origin.staging"
  "vw_": "qraft_origin.mart"
```

**ê²°ê³¼:**
```
Task ID                              â†’ Dataset URN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
transform_data.dim_holiday.run       â†’ dbt:qraft_origin.core.dim_holiday
transform_data.stg_us_sec_meta.run   â†’ dbt:qraft_origin.staging.stg_us_sec_meta
transform_data.us_simul_data_run     â†’ dbt:qraft_origin.mart.us_simul_data
```

### 2.4 Domain Pattern Mapping (Cross-Platform Integration)

**ë¬¸ì œ:** Airflow DAG, DBT ëª¨ë¸, Snowflake í…Œì´ë¸”ì´ ì„œë¡œ ë‹¤ë¥¸ Domainì— ì†í•´ lineage ì¶”ì  ì–´ë ¤ì›€.

**í•´ê²°:** DAG ID íŒ¨í„´ ê¸°ë°˜ ìë™ Domain í• ë‹¹

```python
# metadata_utils.py:387-410
def extract_domain(
    self, dag_id: str, domain_pattern_mapping: Dict[str, str]
) -> Optional[str]:
    """
    Extract domain from DAG ID using pattern mapping
    
    Args:
        dag_id: DAG identifier
        domain_pattern_mapping: Regex patterns â†’ Domain URN mapping
    
    Returns:
        Domain URN or None
    """
    import re
    
    for pattern, domain_urn in domain_pattern_mapping.items():
        if re.search(pattern, dag_id, re.IGNORECASE):
            logger.debug(
                f"DAG '{dag_id}' matched pattern '{pattern}' â†’ domain '{domain_urn}'"
            )
            return domain_urn
    
    logger.debug(f"DAG '{dag_id}' matched no domain patterns")
    return None
```

**ì„¤ì • ì˜ˆì‹œ:**
```yaml
domain_pattern_mapping:
  # Portfolio rebalancing DAGs
  ".*rebalancing.*": "urn:li:domain:Portfolio"
  ".*port_.*": "urn:li:domain:Portfolio"
  # Index data DAGs
  ".*index.*": "urn:li:domain:Index"
  ".*qqq.*": "urn:li:domain:Index"
  # FRED economic data DAGs
  ".*fred.*": "urn:li:domain:FRED"
  # Employee data DAGs
  ".*flex.*": "urn:li:domain:Employee"
  # Security/Simul data DAGs
  ".*simul.*": "urn:li:domain:Security"
  # Core dimension DAGs
  ".*ticker.*": "urn:li:domain:Core"
```

**Cross-Platform ì¼ê´€ì„±:**

| Domain | Airflow DAG Pattern | DBT Model | Snowflake Schema |
|--------|---------------------|-----------|------------------|
| Portfolio | `.*rebalancing.*` | `port_*` | `mart.port_*` |
| Index | `.*index.*`, `.*qqq.*` | `index_*` | `invesco.*` |
| FRED | `.*fred.*` | FRED ëª¨ë¸ | `fred.*` |
| Employee | `.*flex.*` | `dim_flex*` | `employee.*` |
| Security | `.*simul.*` | `int_us_simul_*` | `mart.us_sec_*` |
| Core | `.*ticker.*`, `.*dim_.*` | `dim_*` | `core.*` |

### 2.5 Asset Metadata ìƒì† (Tags & Ownership)

**ë¬¸ì œ:** Assetì€ DAGì˜ tagsì™€ owner ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìƒì†ë°›ì•„ì•¼ í•¨.

**êµ¬í˜„:**

```python
# airflow_source.py:620-732
def _extract_assets(self) -> Iterable[MetadataWorkUnit]:
    """Extract Airflow Assets with tag/owner inheritance"""
    
    for asset_data in self.api_client.get_assets():
        asset_uri = asset_data.get("uri", "unknown")
        asset_urn = self.transformer.make_asset_urn(asset_uri)
        
        # Get asset lineage to find producing DAGs
        events = self.api_client.get_asset_events(asset_uri)
        producing_dags = set()
        dag_tags_list = []
        dag_owners_list = []
        
        for event in events:
            dag_id = event.get("source_dag_id")
            if dag_id:
                producing_dags.add(dag_id)
        
        # Fetch DAG metadata for producing DAGs
        for dag_id in producing_dags:
            dag_data = self.api_client.get_dag(dag_id)
            if dag_data:
                # Extract tags from DAG
                tags = dag_data.get("tags", [])
                for tag in tags:
                    tag_name = tag.get("name") if isinstance(tag, dict) else str(tag)
                    if tag_name:
                        dag_tags_list.append(tag_name)
                
                # Extract owner from DAG
                owner = dag_data.get("owner") or dag_data.get("owners", [])
                if owner:
                    owners = [owner] if isinstance(owner, str) else owner
                    dag_owners_list.extend(owners)
        
        # Emit tags from producing DAGs
        if dag_tags_list:
            asset_tags = self.transformer.extract_tags({"tags": dag_tags_list})
            if asset_tags:
                yield self._make_workunit(
                    entity_urn=asset_urn,
                    aspect=asset_tags,
                    aspect_name="globalTags",
                )
        
        # Emit ownership (deduplicate owners)
        if dag_owners_list:
            unique_owners = list(dict.fromkeys(dag_owners_list))
            asset_ownership = self.transformer.extract_ownership(
                {"owner": unique_owners}
            )
            if asset_ownership:
                yield self._make_workunit(
                    entity_urn=asset_urn,
                    aspect=asset_ownership,
                    aspect_name="ownership",
                )
```

**í•µì‹¬:**
1. Assetì˜ producing DAGë“¤ì„ ì¡°íšŒ
2. ê° DAGì˜ tagsì™€ ownersë¥¼ ìˆ˜ì§‘
3. ì¤‘ë³µ ì œê±° í›„ Assetì— ë™ì¼í•œ tags/ownership ì ìš©
4. í•˜ë‚˜ì˜ Assetì„ ì—¬ëŸ¬ DAGê°€ ìƒì„±í•˜ëŠ” ê²½ìš°, ëª¨ë“  DAGì˜ tags/ownersê°€ í•©ì³ì§

### 2.6 DataProcessInstance (ì‹¤í–‰ ì´ë ¥ ì¶”ì )

**ë¬¸ì œ:** DAG ì‹¤í–‰ ì´ë ¥ì„ DataHubì— ìƒì„¸íˆ ê¸°ë¡í•´ì•¼ í•¨.

**êµ¬í˜„:**

```python
# airflow_source.py:785-902
def _extract_dag_process_instances(self, dag_id: str):
    """Extract DAG runs as DataProcessInstance entities"""
    
    start_date, _ = self.api_client.get_execution_date_range()
    run_count = 0
    
    for dag_run in self.api_client.get_dag_runs(dag_id, start_date):
        if run_count >= self.config.process_instance_max_runs:
            break
        
        run_count += 1
        dag_run_id = dag_run.get("dag_run_id")
        state = dag_run.get("state", "unknown")
        
        # Create DataProcessInstance URN
        dag_urn = self.transformer.make_dag_urn(dag_id)
        instance_id = f"{dag_id}_{dag_run_id}"
        instance_urn = make_data_process_instance_urn(
            orchestrator=self.transformer.platform,
            id=instance_id,
            cluster=self.transformer.platform_instance or self.config.env,
        )
        
        # Properties
        properties = DataProcessInstancePropertiesClass(
            name=dag_run_id,
            externalUrl=self.transformer.make_dag_run_url(
                dag_id, dag_run_id, self.config.airflow_url
            ),
            customProperties={
                "dag_id": dag_id,
                "dag_run_id": dag_run_id,
                "state": state,
                "run_type": dag_run.get("run_type", "unknown"),
                "conf": str(dag_run.get("conf", {})),
            },
        )
        yield self._make_workunit(
            entity_urn=instance_urn,
            aspect=properties,
            aspect_name="dataProcessInstanceProperties",
        )
        
        # Relationships (link to parent DataFlow)
        relationships = DataProcessInstanceRelationshipsClass(
            parentTemplate=dag_urn,
        )
        yield self._make_workunit(
            entity_urn=instance_urn,
            aspect=relationships,
            aspect_name="dataProcessInstanceRelationships",
        )
        
        # Run Event with status
        start_ts = parse_airflow_timestamp(dag_run.get("start_date"))
        end_ts = parse_airflow_timestamp(dag_run.get("end_date"))
        
        # Map Airflow state to DataHub status
        status_map = {
            "success": DataProcessRunStatusClass.COMPLETE,
            "running": DataProcessRunStatusClass.STARTED,
            "failed": DataProcessRunStatusClass.COMPLETE,
            "queued": DataProcessRunStatusClass.STARTED,
        }
        run_status = status_map.get(state, DataProcessRunStatusClass.COMPLETE)
        
        # Result for completed runs
        result = None
        if state in ("success", "failed"):
            result_type = (
                DataProcessInstanceRunResultClass.SUCCESS
                if state == "success"
                else DataProcessInstanceRunResultClass.FAILURE
            )
            result = DataProcessInstanceRunResultClass(
                type=result_type,
                nativeResultType=state,
            )
        
        run_event = DataProcessInstanceRunEventClass(
            status=run_status,
            timestampMillis=start_ts or int(datetime.now().timestamp() * 1000),
            result=result,
        )
        yield self._make_workunit(
            entity_urn=instance_urn,
            aspect=run_event,
            aspect_name="dataProcessInstanceRunEvent",
        )
```

**ì¶”ì¶œ ì •ë³´:**
- DAG Run ID, ìƒíƒœ (success/failed/running)
- ì‹¤í–‰ ì‹œì‘/ì¢…ë£Œ ì‹œê°„
- ì‹¤í–‰ ìœ í˜• (scheduled/manual/backfill)
- ì‹¤í–‰ ì„¤ì • (conf)
- Airflow UI ë§í¬

## 3. Trial & Error

### 3.1 Task Lineage ë¯¸í‘œì‹œ ë¬¸ì œ

**ì¦ìƒ:**
- DataHub UIì—ì„œ DAG ë‚´ë¶€ task ê°„ ì—°ê²°ì„ ì´ í‘œì‹œë˜ì§€ ì•ŠìŒ
- Asset outletsë§Œ í‘œì‹œë¨ (17ê°œ lineage)

**ì›ì¸:**
- Airflow 3.xì˜ `serialized_dag` êµ¬ì¡° ë³€ê²½ìœ¼ë¡œ REST APIì—ì„œ `upstream_task_ids` í•„ë“œ ëˆ„ë½
- DataHubê°€ task dependenciesë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•¨

**ì‹œë„:**
1. **REST APIë§Œ ì‚¬ìš© (ì‹¤íŒ¨)**
   - `/api/v2/dags/{dag_id}/tasks` ì‘ë‹µì— `upstream_task_ids` ì—†ìŒ
   - Task dependencies ì •ë³´ ë¶€ì¡±

2. **Metadata DB ì§ì ‘ ì¿¼ë¦¬ (ì„±ê³µ)**
   - `serialized_dag` í…Œì´ë¸” ì§ì ‘ ì¡°íšŒ
   - `__var` êµ¬ì¡° íŒŒì‹±í•˜ì—¬ `downstream_task_ids` ì¶”ì¶œ
   - ì—­ìˆœìœ¼ë¡œ ë³€í™˜í•˜ì—¬ `upstream_deps` ìƒì„±

**í•´ê²°:**
```python
# 1. Metadata DB í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€
self.metadata_db_client = AirflowMetadataDBClient(
    host=config.metadata_db_host,
    port=config.metadata_db_port,
    database=config.metadata_db_database,
    username=config.metadata_db_username,
    password=config.metadata_db_password,
)

# 2. DAG ì²˜ë¦¬ ì‹œ metadata DB ë°ì´í„° ë¡œë“œ
self._load_metadata_db_data(dag_id)

# 3. Task lineage êµ¬ì„± ì‹œ DB ë°ì´í„° ìš°ì„  ì‚¬ìš©
upstream_task_ids = []
if dag_id in self._dag_dependencies_cache:
    upstream_task_ids = self._dag_dependencies_cache[dag_id].get(task_id, [])
```

**ê²°ê³¼:**
- 17 (asset outletsë§Œ) â†’ **79 (task deps + asset outlets)**
- Task ê°„ lineage ì •ìƒ í‘œì‹œ

### 3.2 Cosmos DBT Outlet ë²„ê·¸

**ì¦ìƒ:**
- `infer_cosmos_outlets: true` ì„¤ì • ì‹œ non-Cosmos taskì˜ ëª…ì‹œì  outletì´ ë¬´ì‹œë¨
- Airflow metadata DBì—ëŠ” outlet ì¡´ì¬í•˜ì§€ë§Œ DataHubì— ì—°ê²° ì•ˆ ë¨

**ì›ì¸:**
```python
# ë²„ê·¸ ì½”ë“œ (airflow_source.py:377-387)
skip_explicit_outlets = False
if self.config.infer_cosmos_outlets:
    is_cosmos_task = bool(re.match(cosmos_pattern, task_id))
    skip_explicit_outlets = not is_cosmos_task  # â† ë²„ê·¸!
```

- Cosmos taskê°€ ì•„ë‹ˆë©´ ëª…ì‹œì  outletì„ ê±´ë„ˆë›°ëŠ” ë¡œì§
- Cosmos ì¶”ë¡ ì´ ëª…ì‹œì  outletì„ **ëŒ€ì²´**í•˜ëŠ” ê²ƒìœ¼ë¡œ ì˜ëª» êµ¬í˜„ë¨

**í•´ê²°:**
```python
# ìˆ˜ì •ëœ ì½”ë“œ
# Always extract explicit outlets from metadata DB for all tasks
# (Cosmos inference adds DBT outlets additionally, doesn't replace explicit ones)
if dag_id in self._dag_outlets_cache:
    outlet_uris = self._dag_outlets_cache[dag_id].get(task_id, [])
    for uri in outlet_uris:
        dataset_urn = self.transformer.make_asset_urn(uri)
        outlet_dataset_urns.append(dataset_urn)

# Infer Cosmos DBT outlets if enabled (ADDITIONAL, not replacement)
if self.config.infer_cosmos_outlets:
    cosmos_outlet = self._infer_cosmos_outlet(task_id)
    if cosmos_outlet:
        outlet_dataset_urns.append(cosmos_outlet)
```

**í•µì‹¬:**
- Cosmos ì¶”ë¡ ì€ **ì¶”ê°€** ê¸°ëŠ¥ (ëª…ì‹œì  outlet **ëŒ€ì²´** ì•„ë‹˜)
- ëª¨ë“  taskì—ì„œ ëª…ì‹œì  outlet ë¨¼ì € ì¶”ì¶œ
- Cosmos íŒ¨í„´ ë§¤ì¹­ ì‹œ DBT outlet **ì¶”ê°€**ë¡œ ë§ë¶™ì„

### 3.3 Airflow 3.1 Cosmos í˜¸í™˜ì„± ë¬¸ì œ

**ì¦ìƒ:**
- Airflow 3.0.6ì—ì„œ Cosmos task ì‹¤í–‰ ì‹œ import error
- `Cosmos requires Airflow >= 3.1.0`

**ì›ì¸:**
- Cosmos í”ŒëŸ¬ê·¸ì¸ì´ Airflow 3.1 ì´ìƒ ìš”êµ¬
- Asset URI í˜•ì‹ ë³€ê²½ í•„ìš”

**ì‹œë„:**
1. **Cosmos ë‹¤ìš´ê·¸ë ˆì´ë“œ (ì‹¤íŒ¨)**
   - êµ¬ë²„ì „ CosmosëŠ” Airflow 3.x ë¯¸ì§€ì›

2. **Airflow ì—…ê·¸ë ˆì´ë“œ (ì„±ê³µ)**
   - Airflow 3.0.6 â†’ 3.1.3 ì—…ê·¸ë ˆì´ë“œ
   - `AIRFLOW__COSMOS__USE_DATASET_AIRFLOW3_URI_STANDARD=True` ì„¤ì •
   - Asset URI: `dot notation` â†’ `slash notation` ë³€ê²½

**í•´ê²°:**
```bash
# requirements.txt
apache-airflow==3.1.3
astronomer-cosmos==1.11.1

# .env
AIRFLOW__COSMOS__USE_DATASET_AIRFLOW3_URI_STANDARD=True
```

**ê²°ê³¼:**
- Cosmos DBT task ì •ìƒ ì‹¤í–‰
- Asset lineage ìë™ ìƒì„±

### 3.4 Plugin Import ê²½ë¡œ ë¬¸ì œ

**ì¦ìƒ:**
- `attempted relative import beyond top-level package`
- Airflowê°€ plugins ë””ë ‰í† ë¦¬ë¥¼ ì œëŒ€ë¡œ ì¸ì‹í•˜ì§€ ëª»í•¨

**ì›ì¸:**
- Pluginsì˜ `__init__.py`ì—ì„œ ìƒëŒ€ import ì‚¬ìš©
- Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì‹œ `__pycache__` stale bytecode ë¬¸ì œ

**ì‹œë„:**
1. **PYTHONPATH ì¶”ê°€ (ë¶€ë¶„ ì„±ê³µ)**
   - `export PYTHONPATH=/opt/airflow/plugins:$PYTHONPATH`
   - ì¼ë¶€ importëŠ” ë™ì‘í•˜ì§€ë§Œ ì—¬ì „íˆ ê²½ê³  ë¡œê·¸

2. **ì ˆëŒ€ importë¡œ ë³€ê²½ (ì„±ê³µ)**
   ```python
   # Before (relative import)
   from .custom_keycloak_auth_manager import CustomKeycloakAuthManager
   
   # After (absolute import)
   from airflow_keycloak.custom_keycloak_auth_manager import CustomKeycloakAuthManager
   ```

3. **__pycache__ ìë™ ì •ë¦¬ (ì™„ì „ í•´ê²°)**
   ```dockerfile
   # Dockerfile
   RUN find /opt/airflow/plugins -type d -name "__pycache__" -exec rm -rf {} + || true
   ```

**ê²°ê³¼:**
- Import ê²½ê³  ë¡œê·¸ ì™„ì „ ì œê±°
- Plugins ì •ìƒ ë¡œë“œ

## 4. í™˜ê²½ë³„ ì„¤ì •

### Local í™˜ê²½

```yaml
# airflow_local.yml
source:
  type: datahub_airflow_source.airflow_source.AirflowSource
  config:
    airflow_url: "http://host.docker.internal:8082"
    
    # Keycloak OIDC
    keycloak_server: "https://auth.qraft.ai"
    keycloak_realm: "qraft"
    keycloak_client_id: "data-pipeline-keycloak-client"
    keycloak_client_secret: "${AIRFLOW_VAR_KEYCLOAK_CLIENT_SECRET}"
    
    # Environment
    env: "LOCAL"
    platform_instance: "airflow-local"
    
    # Metadata DB
    metadata_db_enabled: true
    metadata_db_host: "host.docker.internal"
    metadata_db_port: 5432
    metadata_db_database: "airflow"
    metadata_db_username: "airflow"
    metadata_db_password: "${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_PASSWORD}"
    
    # Capture options
    capture_executions: true
    capture_executions_days: 7
    capture_assets: true
    
    # Advanced
    verify_ssl: false
    timeout: 30
```

### Prod í™˜ê²½

```yaml
# airflow_prod.yml
source:
  type: datahub_airflow_source.airflow_source.AirflowSource
  config:
    airflow_url: "https://airflow.prod.qraft.ai"
    
    # Keycloak OIDC
    keycloak_server: "https://auth.qraft.ai"
    keycloak_client_secret: "${KEYCLOAK_CLIENT_SECRET_PROD}"
    
    # Environment
    env: "PROD"
    platform_instance: "airflow-prod"
    
    # Metadata DB
    metadata_db_enabled: true
    metadata_db_host: "postgres.prod.qraft.ai"
    
    # Capture options
    capture_executions_days: 30
    emit_process_instances: true
    process_instance_max_runs: 10
    
    # Advanced
    verify_ssl: true
    timeout: 45
    max_workers: 10

sink:
  type: datahub-rest
  config:
    server: "${DATAHUB_GMS_URL_PROD}"
    token: "${DATAHUB_ACCESS_TOKEN_PROD}"
```

## 5. ì„±ëŠ¥ ìµœì í™”

### 5.1 Pagination ë° Caching

```python
# api_client.py:85-141
def _paginate(
    self, endpoint: str, params: Optional[Dict[str, Any]] = None, limit: int = 100
) -> Iterator[Dict[str, Any]]:
    """Paginate through API results"""
    params = params or {}
    offset = 0
    
    while True:
        params["limit"] = limit
        params["offset"] = offset
        
        response = self._request("GET", endpoint, params=params)
        data = response.json()
        
        # Handle different pagination response formats
        items = (
            data.get("dags")
            or data.get("dag_runs")
            or data.get("task_instances")
            or data.get("assets")
            or []
        )
        
        if not items:
            break
        
        for item in items:
            yield item
        
        # Check if there are more pages
        total_entries = data.get("total_entries", 0)
        if offset + limit >= total_entries:
            break
        
        offset += limit
```

### 5.2 Metadata DB ìºì‹±

```python
# airflow_source.py:132-134
# Cache for metadata DB data per DAG
self._dag_dependencies_cache: Dict[str, Dict[str, List[str]]] = {}
self._dag_outlets_cache: Dict[str, Dict[str, List[str]]] = {}

# airflow_source.py:392-424
def _load_metadata_db_data(self, dag_id: str) -> None:
    """Load metadata DB data for a DAG into cache"""
    if not self.metadata_db_client:
        return
    
    if dag_id in self._dag_dependencies_cache:
        return  # Already loaded
    
    try:
        # Load task dependencies
        dependencies = self.metadata_db_client.get_task_dependencies(dag_id)
        self._dag_dependencies_cache[dag_id] = dependencies
        
        # Load task outlets
        outlets = self.metadata_db_client.get_task_outlets(dag_id)
        self._dag_outlets_cache[dag_id] = outlets
        
    except Exception as e:
        logger.warning(f"Failed to load metadata DB data for DAG '{dag_id}': {e}")
        self._dag_dependencies_cache[dag_id] = {}
        self._dag_outlets_cache[dag_id] = {}
```

**íš¨ê³¼:**
- DAGë‹¹ 1ë²ˆë§Œ DB ì¿¼ë¦¬
- Task ìˆ˜ë°± ê°œì¸ DAGë„ ë¹ ë¥´ê²Œ ì²˜ë¦¬
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”

## 6. ë²„ì „ íˆìŠ¤í† ë¦¬

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ ë‚´ìš© |
|------|------|-----------|
| 1.2.0 | 2025-11-27 | DataProcessInstance, Task log URL, Connection/Variable, SLA ì¶”ì¶œ ê¸°ëŠ¥ ì¶”ê°€ |
| 1.1.0 | 2025-11-27 | Domain Pattern Mapping ê¸°ëŠ¥ ì¶”ê°€ (Cross-Platform Integration) |
| 1.0.1 | 2025-11-26 | Cosmos DBT Outlet ì¶”ë¡  ê¸°ëŠ¥ ì¶”ê°€, Asset outlet ë²„ê·¸ ìˆ˜ì • |
| 1.0.0 | 2025-11-22 | ì´ˆê¸° ë¦´ë¦¬ìŠ¤ - Airflow 3.x REST API ê¸°ë°˜ ì»¤ë„¥í„° |

## ğŸ“ Related

### Projects ë°°ê²½ (Why)
- [[02-Areas/í¬ë˜í”„íŠ¸í…Œí¬ë†€ë¡œì§€ìŠ¤/Projects/03-ì¸í”„ë¼êµ¬ì¶•-Infrastructure/Airflow-3.0-ì—…ê·¸ë ˆì´ë“œ-ë°°ê²½|Airflow-3.0-ì—…ê·¸ë ˆì´ë“œ-ë°°ê²½]] - ì™œ Airflow 3.0ì„ ë„ì…í–ˆëŠ”ê°€
- [[02-Areas/í¬ë˜í”„íŠ¸í…Œí¬ë†€ë¡œì§€ìŠ¤/Projects/03-ì¸í”„ë¼êµ¬ì¶•-Infrastructure/DataHub-ì»¤ìŠ¤í…€-êµ¬í˜„-ìƒì„¸|DataHub-ì»¤ìŠ¤í…€-êµ¬í˜„-ìƒì„¸]] - DataHub Custom Connector í”„ë¡œì íŠ¸ ë°°ê²½

### Technology (Core Concepts)
- [[Airflow]] - Airflow ê¸°ë³¸ ê°œë… ë° Qraft ì ìš© ì‚¬ë¡€
- [[DataHub]] - DataHub ë©”íƒ€ë°ì´í„° ì¹´íƒˆë¡œê·¸

### Technology (Related Implementation)
- [[Keycloak-OIDC-ì¸ì¦]] - OIDC ì¸ì¦ êµ¬í˜„
- [[DBT-êµ¬í˜„]] - DBT í†µí•© êµ¬í˜„
- [[TransferPipeline-íŒ¨í„´]] - ë°ì´í„° ì „ì†¡ íŒ¨í„´

### Projects (ì‹¤ì œ ì‚¬ìš©)
- [[02-Areas/í¬ë˜í”„íŠ¸í…Œí¬ë†€ë¡œì§€ìŠ¤/Projects/07-ê±°ë²„ë„ŒìŠ¤-Governance/DataHub-ë„ì…|DataHub-ë„ì…]] - DataHub ë„ì… í”„ë¡œì íŠ¸
- [[02-Areas/í¬ë˜í”„íŠ¸í…Œí¬ë†€ë¡œì§€ìŠ¤/Projects/07-ê±°ë²„ë„ŒìŠ¤-Governance/ë°ì´í„°-ê±°ë²„ë„ŒìŠ¤-ì „ëµ-ìˆ˜ë¦½|ë°ì´í„°-ê±°ë²„ë„ŒìŠ¤-ì „ëµ-ìˆ˜ë¦½]] - ê±°ë²„ë„ŒìŠ¤ ì „ëµ

---

**ì‘ì„±ì¼:** 2025-11-30  
**ì¹´í…Œê³ ë¦¬:** #Technology #Airflow #DataPlatform #Metadata  
**íƒœê·¸:** #Airflow3 #CustomConnector #DataHub #AssetLineage
