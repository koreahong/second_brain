---
title: TransferPipeline íŒ¨í„´
type: technical-pattern
tags:
  - airflow
  - adapter-pattern
  - data-pipeline
  - etl
  - python
created: '2025-11-30'
updated: '2025-11-30'
status: evergreen
---
# TransferPipeline íŒ¨í„´

## ğŸ“‹ ê°œìš”

TransferPipelineì€ **Adapter íŒ¨í„´**ì„ í™œìš©í•œ ë°ì´í„° ì „ì†¡ ì¶”ìƒí™” ë ˆì´ì–´ì…ë‹ˆë‹¤. Source(ì¶œì²˜) â†’ Target(ëª©ì ì§€) ê°„ ë°ì´í„° ì´ë™ì„ í‘œì¤€í™”í•˜ì—¬, ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì„ ì¡°í•©í•  ìˆ˜ ìˆëŠ” ìœ ì—°í•œ êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**í•µì‹¬ ê°œë…:**
- **SourceAdapter**: ë°ì´í„°ë¥¼ ì½ëŠ” ì—­í•  (SFTP, FTP, API ë“±)
- **TargetAdapter**: ë°ì´í„°ë¥¼ ì“°ëŠ” ì—­í•  (Snowflake, NAS, S3 ë“±)
- **TransferPipeline**: Sourceì™€ Targetì„ ì—°ê²°í•˜ëŠ” íŒŒì´í”„ë¼ì¸

## ğŸ¯ Adapter íŒ¨í„´ ì ìš©

### ì¸í„°í˜ì´ìŠ¤ ì •ì˜

```python
# plugins/airflow_transfer/interfaces.py

from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Optional, Callable, Dict, Any

class SourceAdapter(ABC):
    """ë°ì´í„° ì½ê¸° ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def list_files(self) -> List[str]:
        """íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        pass
    
    @abstractmethod
    def read_file(self, remote_path: str, local_path: Path):
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        pass
    
    def open(self):
        """ì—°ê²° ì—´ê¸° (ì„ íƒ)"""
        pass
    
    def close(self):
        """ì—°ê²° ë‹«ê¸° (ì„ íƒ)"""
        pass


class TargetAdapter(ABC):
    """ë°ì´í„° ì“°ê¸° ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def write_data(self, local_path: Path, dest_path: str, **kwargs):
        """ë°ì´í„° ì ì¬"""
        pass
    
    def open(self):
        """ì—°ê²° ì—´ê¸° (ì„ íƒ)"""
        pass
    
    def close(self):
        """ì—°ê²° ë‹«ê¸° (ì„ íƒ)"""
        pass
```

### Pipeline êµ¬í˜„

```python
# plugins/airflow_transfer/transfer_pipeline.py

class TransferPipeline:
    def __init__(self, source: SourceAdapter, target: TargetAdapter):
        self.source = source
        self.target = target
    
    def transfer(
        self,
        dest_base: str = "",
        context_provider: Optional[Callable[[str], Dict[str, Any]]] = None,
        transformer: Optional[Callable[[Path], Path]] = None,
    ):
        """
        Source â†’ Target ë°ì´í„° ì „ì†¡
        
        Args:
            dest_base: ëŒ€ìƒ ê²½ë¡œ
            context_provider: íŒŒì¼ë³„ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (í…Œì´ë¸”ëª…, ì»¬ëŸ¼ ë“±)
            transformer: íŒŒì¼ ë³€í™˜ í•¨ìˆ˜ (JSONâ†’Parquet ë“±)
        """
        # 1. ì—°ê²° ì—´ê¸°
        if hasattr(self.source, "open"):
            self.source.open()
        if hasattr(self.target, "open"):
            self.target.open()
        
        # 2. íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        files = self.source.list_files()
        
        try:
            for remote_path in files:
                filename = os.path.basename(remote_path)
                
                # 3. ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                context = context_provider(filename) if context_provider else {}
                
                # 4. ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
                tmp_path = Path(tempfile.gettempdir()) / filename
                self.source.read_file(remote_path, tmp_path)
                
                # 5. ë³€í™˜ (ì˜µì…˜)
                upload_path = transformer(tmp_path) if transformer else tmp_path
                
                # 6. íƒ€ê²Ÿì— ì—…ë¡œë“œ
                dest_path = context.get("dest_path", os.path.join(dest_base, filename))
                self.target.write_data(upload_path, dest_path, **context)
                
                # 7. ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if tmp_path.exists():
                    tmp_path.unlink()
        
        finally:
            # 8. ì—°ê²° ë‹«ê¸°
            if hasattr(self.source, "close"):
                self.source.close()
            if hasattr(self.target, "close"):
                self.target.close()
```

## ğŸ”Œ êµ¬í˜„ëœ Adapterë“¤

### SourceAdapter êµ¬í˜„ì²´

#### 1. SFTPAdapter
```python
from plugins.airflow_transfer.adapters import SFTPAdapter

source = SFTPAdapter(
    conn_id="sftp-vendor",
    base_path="/data/files",
    filter_fn=lambda f: f.endswith(".csv")
)
```

#### 2. TickHistoryAdapter (LSEG DSS API)
```python
from plugins.airflow_transfer.adapters import TickHistoryAdapter

source = TickHistoryAdapter(
    schedule_name="daily_qraft_data_futures",
    extracted_file_name="daily_qraft_data_futures_20250129.csv"
)
```

#### 3. KRXApiAdapter
```python
from plugins.airflow_transfer.adapters import KRXApiAdapter

source = KRXApiAdapter(
    api_key=Variable.get("krx_api_key"),
    endpoint="/equities/ohlcv"
)
```

### TargetAdapter êµ¬í˜„ì²´

#### 1. SnowflakeAdapter

**3ê°€ì§€ ì ì¬ ë°©ì‹ ì§€ì›:**

##### ë°©ì‹ 1: Direct Insert (ê¸°ë³¸)
```python
target = SnowflakeAdapter(
    conn_id="snowflake-account-etl",
    database="QRAFT_ORIGIN",
    table_name="raw_data"
)

# DataFrameì„ ë°”ë¡œ INSERT
pipeline.transfer()
```

##### ë°©ì‹ 2: Stage-Copy (ëŒ€ìš©ëŸ‰ ì¼ê´„ ì ì¬)
```python
def get_context(filename):
    return {
        "use_stage_copy": True,
        "table_name": "QRAFT_ORIGIN.LSEG_DSS.FUTURES_1D_TEMP",
        "stage_name": "@QRAFT_ORIGIN.LSEG_DSS.TICKHISTORY",
        "columns": ["TRADE_DATE", "RIC", "OPEN", "HIGH", "LOW", "CLOSE"],
        "metadata_columns": {
            "created_at": "2025-01-29 10:00:00"
        },
        "truncate_before_copy": True
    }

pipeline.transfer(context_provider=get_context)
```

**ì²˜ë¦¬ íë¦„:**
```
1. íŒŒì¼ â†’ Stage ì—…ë¡œë“œ (PUT)
2. ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€ (created_at ë“±)
3. TRUNCATE TABLE (ì˜µì…˜)
4. COPY INTOë¡œ ì ì¬
```

##### ë°©ì‹ 3: Stage-Merge (Upsert)
```python
def get_context(filename):
    return {
        "use_stage_merge": True,
        "table_name": "QRAFT_ORIGIN.CORE.DIM_TICKER",
        "stage_name": "@FILES",
        "columns": ["TICKER", "NAME", "SECTOR"],
        "comp_cols": ["TICKER"],  # MERGE ë¹„êµ ì»¬ëŸ¼
        "transform_fn": lambda df: preprocess_ticker(df)
    }

pipeline.transfer(context_provider=get_context)
```

**ì²˜ë¦¬ íë¦„:**
```
1. íŒŒì¼ â†’ Stage ì—…ë¡œë“œ
2. ë³€í™˜ í•¨ìˆ˜ ì ìš© (ì˜µì…˜)
3. COPY INTO temp í…Œì´ë¸”
4. MERGE (comp_cols ê¸°ì¤€)
   - ì¡´ì¬í•˜ë©´ UPDATE
   - ì—†ìœ¼ë©´ INSERT
```

#### 2. NASAdapter
```python
target = NASAdapter(
    base_path="/mnt/nas-quant/short-term/krx/equities/ohlcv/daily/raw"
)
```

## ğŸ“ ì‚¬ìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: SFTP â†’ Snowflake (ì¼ê´„ ì ì¬)

```python
from plugins.airflow_transfer.adapters import SFTPAdapter, SnowflakeAdapter
from plugins.airflow_transfer.transfer_pipeline import TransferPipeline

@task
def load_vendor_data(**kwargs):
    # Source: SFTP
    source = SFTPAdapter(
        conn_id="sftp-vendor",
        base_path="/outbound",
        filter_fn=lambda f: f.startswith("daily_") and f.endswith(".csv")
    )
    
    # Target: Snowflake
    target = SnowflakeAdapter(
        conn_id="snowflake-account-etl",
        database="QRAFT_ORIGIN"
    )
    
    # Context: íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
    def get_context(filename: str) -> dict:
        # daily_20250129.csv â†’ 20250129
        date_str = filename.split("_")[1].split(".")[0]
        
        return {
            "use_stage_copy": True,
            "table_name": "QRAFT_ORIGIN.VENDOR.RAW_DATA",
            "stage_name": "@FILES",
            "columns": ["DATE", "SYMBOL", "PRICE", "VOLUME", "CREATED_AT"],
            "metadata_columns": {
                "date": date_str,
                "created_at": pendulum.now().to_datetime_string()
            },
            "truncate_before_copy": False  # Append
        }
    
    # Pipeline ì‹¤í–‰
    pipeline = TransferPipeline(source, target)
    copied = pipeline.transfer(context_provider=get_context)
    
    logger.info(f"âœ… {copied} files transferred")
```

### ì˜ˆì‹œ 2: API â†’ Snowflake (ë³€í™˜ í¬í•¨)

```python
import pandas as pd
from pathlib import Path

@task
def fetch_krx_ohlcv(**kwargs):
    source = KRXApiAdapter(
        api_key=Variable.get("krx_api_key"),
        endpoint="/equities/ohlcv"
    )
    
    target = SnowflakeAdapter(
        conn_id="snowflake-account-etl",
        database="QRAFT_ORIGIN"
    )
    
    # Transformer: JSON â†’ Parquet ë³€í™˜
    def json_to_parquet(json_path: Path) -> Path:
        df = pd.read_json(json_path)
        
        # ë°ì´í„° ë³€í™˜
        df['TRADE_DATE'] = pd.to_datetime(df['date'])
        df['TICKER'] = df['symbol']
        
        # Parquet ì €ì¥
        parquet_path = json_path.with_suffix('.parquet')
        df.to_parquet(parquet_path, index=False)
        
        return parquet_path
    
    def get_context(filename: str) -> dict:
        return {
            "use_stage_copy": True,
            "table_name": "QRAFT_ORIGIN.KRX.EQUITIES_OHLCV_DAILY",
            "stage_name": "@FILES",
            "columns": ["TRADE_DATE", "TICKER", "OPEN", "HIGH", "LOW", "CLOSE", "VOLUME"],
            "file_format": "TYPE = 'PARQUET'"
        }
    
    pipeline = TransferPipeline(source, target)
    pipeline.transfer(context_provider=get_context, transformer=json_to_parquet)
```

### ì˜ˆì‹œ 3: SFTP â†’ NAS (íŒŒì¼ ë°±ì—…)

```python
@task
def backup_to_nas(**kwargs):
    source = SFTPAdapter(
        conn_id="sftp-lseg",
        base_path="/tickhistory/delivered",
        filter_fn=lambda f: f.endswith(".csv.gz")
    )
    
    target = NASAdapter(
        base_path="/mnt/nas-quant/backup/lseg/tickhistory"
    )
    
    pipeline = TransferPipeline(source, target)
    pipeline.transfer()
```

## âš ï¸ íŠ¸ë¼ì´ ì—ëŸ¬

### ë¬¸ì œ 1: ëŒ€ìš©ëŸ‰ íŒŒì¼ ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ:**
```
MemoryError: Unable to allocate array with shape (10000000, 50)
```

**ì›ì¸:** DataFrame ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ í›„ Snowflake INSERT

**í•´ê²°:**
```python
# âŒ Before: Direct Insert (ë©”ëª¨ë¦¬ ì‚¬ìš©)
df = pd.read_csv(local_path)
target.write_data_direct(df, table_name)

# âœ… After: Stage-Copy (ìŠ¤íŠ¸ë¦¬ë°)
context = {
    "use_stage_copy": True,
    "table_name": "...",
    "stage_name": "@FILES"
}
target.write_data(local_path, **context)
```

**íš¨ê³¼:**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 8GB â†’ 500MB
- ì²˜ë¦¬ ì‹œê°„: 45ë¶„ â†’ 8ë¶„

### ë¬¸ì œ 2: COPY INTO ì¤‘ë³µ ì‹¤í–‰

**ì¦ìƒ:**
```
Duplicate row detected: TICKER='AAPL', DATE='2025-01-29'
```

**ì›ì¸:** 
- `truncate_before_copy=False`ë¡œ Append ëª¨ë“œ
- ì¬ì‹¤í–‰ ì‹œ ë™ì¼ ë°ì´í„° ì¤‘ë³µ ì ì¬

**í•´ê²°:**
```python
# ë°©ë²• 1: Truncate ì‚¬ìš© (ì „ì²´ ì‚­ì œ í›„ ì ì¬)
context = {
    "truncate_before_copy": True  # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
}

# ë°©ë²• 2: Stage-Merge ì‚¬ìš© (Upsert)
context = {
    "use_stage_merge": True,
    "comp_cols": ["TICKER", "TRADE_DATE"]  # ì¤‘ë³µ ì²´í¬ ì»¬ëŸ¼
}
```

### ë¬¸ì œ 3: Metadata ì»¬ëŸ¼ ìˆœì„œ ë¶ˆì¼ì¹˜

**ì¦ìƒ:**
```
SQL compilation error: Column 'CREATED_AT' does not exist in table
```

**ì›ì¸:** `columns` ë¦¬ìŠ¤íŠ¸ì™€ ì‹¤ì œ CSV ì»¬ëŸ¼ ìˆœì„œ ë¶ˆì¼ì¹˜

**í•´ê²°:**
```python
# âœ… ì •í™•í•œ ì»¬ëŸ¼ ìˆœì„œ ì§€ì •
context = {
    "columns": [
        "TRADE_DATE",   # ì›ë³¸ CSV: 1ë²ˆì§¸ ì»¬ëŸ¼
        "TICKER",       # ì›ë³¸ CSV: 2ë²ˆì§¸ ì»¬ëŸ¼
        "PRICE",        # ì›ë³¸ CSV: 3ë²ˆì§¸ ì»¬ëŸ¼
        "CREATED_AT"    # metadata_columnsë¡œ ì¶”ê°€
    ],
    "metadata_columns": {
        "created_at": "2025-01-29"
    }
}
```

**SnowflakeAdapter ë‚´ë¶€ ì²˜ë¦¬:**
```python
# 1. ì›ë³¸ CSV ì½ê¸° (í—¤ë” ì—†ìŒ)
df_original = pd.read_csv(local_path, sep="|", header=None)
# â†’ [0, 1, 2] ì»¬ëŸ¼ (TRADE_DATE, TICKER, PRICE)

# 2. columns ìˆœì„œëŒ€ë¡œ ì¬êµ¬ì„±
result_data = {}
original_col_idx = 0

for col_name in columns:  # ["TRADE_DATE", "TICKER", "PRICE", "CREATED_AT"]
    if col_name.lower() in metadata_columns:
        # ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼
        result_data[col_name] = [metadata_columns[col_name.lower()]] * len(df)
    else:
        # ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼
        result_data[col_name] = df_original[original_col_idx].tolist()
        original_col_idx += 1

# 3. ìƒˆ DataFrame ìƒì„± ë° ì €ì¥
df_new = pd.DataFrame(result_data)
df_new.to_csv(local_path, header=False, sep="|")
```

### ë¬¸ì œ 4: SFTP ì—°ê²° íƒ€ì„ì•„ì›ƒ

**ì¦ìƒ:**
```
paramiko.ssh_exception.SSHException: Timeout opening channel
```

**ì›ì¸:** 
- ëŒ€ëŸ‰ íŒŒì¼ ì „ì†¡ ì¤‘ ì—°ê²° ìœ ì§€ ì‹œê°„ ì´ˆê³¼
- `list_files()` í›„ ì˜¤ë˜ ê±¸ë ¤ì„œ ì—°ê²° ëŠê¹€

**í•´ê²°:**
```python
class SFTPAdapter(SourceAdapter):
    def __init__(self, conn_id, base_path, filter_fn=None, keepalive_interval=60):
        self.keepalive_interval = keepalive_interval
    
    def open(self):
        self.conn = self.hook.get_conn()
        self.sftp = self.conn.open_sftp()
        
        # Keep-alive ì„¤ì •
        transport = self.conn.get_transport()
        transport.set_keepalive(self.keepalive_interval)
    
    def list_files(self):
        # íƒ€ì„ì•„ì›ƒ ë°©ì§€: ì—°ê²° ìƒíƒœ í™•ì¸
        if not self.sftp or not self.conn.get_transport().is_active():
            self.open()
        
        return super().list_files()
```

## ğŸ“ Related

### Projects ë°°ê²½ (Why)
- [[02-Areas/í¬ë˜í”„íŠ¸í…Œí¬ë†€ë¡œì§€ìŠ¤/Projects/03-ì¸í”„ë¼êµ¬ì¶•-Infrastructure/TransferPipeline-ë„ì…-ë°°ê²½|TransferPipeline-ë„ì…-ë°°ê²½]] - ì™œ ì´ íŒ¨í„´ì´ í•„ìš”í–ˆëŠ”ê°€

### Technology (Core Concepts)
- [[Airflow]] - Airflow ê¸°ë³¸ ê°œë… ë° Qraft ì ìš© ì‚¬ë¡€
- [[Snowflake]] - Snowflake Data Warehouse

### Technology (Related Implementation)
- [[Airflow-3.0-êµ¬í˜„]] - Airflow 3.0 í”Œë«í¼ êµ¬í˜„
- [[DBT-êµ¬í˜„]] - DBT ë°ì´í„° ë³€í™˜

### Projects (ì‹¤ì œ ì‚¬ìš©)
- [[02-Areas/í¬ë˜í”„íŠ¸í…Œí¬ë†€ë¡œì§€ìŠ¤/Projects/Active/qraft-data-platform-í†µí•©í”„ë¡œì íŠ¸|qraft-data-platform-í†µí•©í”„ë¡œì íŠ¸]] - Data Platformì—ì„œ ì‹¤ì œ ì‚¬ìš©

---

**ì‘ì„±ì¼**: 2025-11-30
**ì¹´í…Œê³ ë¦¬**: Data Engineering Pattern
**íƒœê·¸**: #airflow #adapter-pattern #data-pipeline #etl
