---
title: Data Governance
type: resource
tags:
  - data-governance
  - policy
created: '2025-11-30'
updated: '2025-11-30'
aliases: []
status: seedling
maturity: 0
---

<details>
<summary>크래프트 데이터 거버넌스 방향성</summary>

## 의도 파악

- 데이터 엔지니어로 이직 후 혼란스러운 데이터 운영 구조(팀별 파이프라인 운영, 중복/비표준화)와
- 조직적 저항(리서치팀이 데이터 통합을 원하지 않음) 속에서
- 실현 가능한 데이터 거버넌스 프레임워크를 어떻게 설계/적용할지 묻는 상황
---

## 권위자의 조언

### 1. 전략적 관점 (조직/프로세스/비즈니스)

1. Top-down 명분 확보
  - “CFO/리스크 관리/컴플라이언스”를 근거로 삼으세요. 금융회사는 규제 준수(감사, 데이터 리텐션, 보안)가 강력한 명분이 됩니다.
  - 즉, “통합”이 아니라 **“투명한 데이터 출처 관리 + 규제 대응”**이 목적이라는 framing이 설득 포인트입니다.
1. 리서치 팀 반감 관리
  - 데이터 통합을 강제하지 말고, 카탈로그와 계보(Lineage)를 제공하는 ‘거울(mirror)’ 역할부터 시작합니다.
  - 즉, “당신들이 쓰는 데이터를 건드리지 않고, 우리가 옆에서 ‘추적·정리’만 해줄게요” → 신뢰 구축
1. 실시간 vs 배치 혼재
  - 전략적으로 데이터 중요도/민감도 기준으로 분류하세요.
  - 예: 규제/재무 데이터(강제 관리 필요), 리서치용 실험 데이터(자율성 부여)
---

### 2. 기술적 관점 (도구/아키텍처/구현 방법)

1. 데이터 카탈로그 도입 (OpenMetadata, DataHub)
  - “모든 걸 중앙화” 대신 각 팀의 파이프라인을 연결(ingest) 하여 메타데이터/라인에지만 통합합니다.
  - 장점: 레거시 코드까지 손대지 않고도 “어떤 데이터가 어디서 왔는지” 보여줄 수 있음
1. 데이터 품질 체크는 최소 단위부터
  - 예: Great Expectations, dbt test 같은 rule 기반 체크를 소스/마트 단계에 붙이고 Slack alert 제공
  - “통제(control)”보다는 “가시성(visibility)”을 강조
1. 아키텍처 접근 방식
  - Phase 1: 데이터 소스 인벤토리 작성 → 메타데이터 카탈로그에 반영
  - Phase 2: 데이터 계보/품질 규칙 추가
  - Phase 3: SLA/표준화 적용, 일부 공통 데이터셋을 관리(예: 기준 코드표, 금융상품 마스터 등)
---

### 3. 실무적 관점 (경험 기반 인사이트)

- 리서치 조직은 **“내 연구를 방해하지 말라”**는 게 본심입니다.
  → 따라서 **“당신의 시간을 줄여주겠다”**라는 메시지가 먹힙니다.

  - 예: “매번 API 문서 뒤지는 대신 카탈로그 검색 한 번으로 소스를 찾을 수 있게 하겠습니다.”
- 규제나 감사 대응 경험이 있는 회사라면 **Audit Trail(누가 어떤 데이터를 언제 썼는지)**을 강조하세요.
  - 이는 리서치팀도 eventually 납득합니다.
---

## 실행 가이드

✅ Step 1: 데이터 인벤토리 (비침해적 방식)

- 각 팀이 쓰는 원천 소스, 적재 방식, 파이프라인 주기를 엑셀/컨플루언스/Notion 템플릿으로 수집
```markdown
| 데이터명 | 소스 채널 | 담당 팀 | 배치/실시간 | 목적 | 저장 위치 | 품질 체크 여부 |
|----------|-----------|---------|--------------|-------|------------|----------------|
| 주문체결 | 금융사 전용 TCP | 리서치1팀 | 실시간 | 전략 백테스트 | NAS 서버 | 없음 |
| KOSPI 일별 | Snowflake | 전략팀 | 배치 | 리스크 관리 | Snowflake | O (중복/결측 체크) |

```

✅ Step 2: 카탈로그/계보 연결

- OpenMetadata ingestion → Snowflake, Postgres, API connector 설정
- “우리는 파이프라인 코드를 건드리지 않는다. 단지 문서화를 자동화한다”
✅ Step 3: 데이터 품질 룰 MVP

- 핵심 데이터셋(리스크/재무 보고용)부터 rule 적용
- 예: 결측률 < 0.1%, 중복 row 없음, 날짜 정합성 확인
✅ Step 4: 공통 데이터셋 정의

- 금융상품 코드표, 고객 ID 매핑 같은 전사 공통 dimension을 먼저 표준화
---

## 추가 고려사항

- 리스크
  - 과도한 중앙화 시도 → 리서치팀 강력 반발
  - 레거시 코드 직접 수정 → 장애 가능성
- 한계점
  - 모든 소스를 단기간에 통합 불가 (점진적 접근 필요)
- 보완책
  - “카탈로그 & 계보”부터 시작 → Visibility 확보 → 신뢰 구축 → 점진적 통합
</details>

---

## 📎 Related

<!-- 자동 생성된 섹션 - 수동으로 링크를 추가하세요 -->

### Projects

### Knowledge

### Insights

