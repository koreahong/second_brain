---
title: nepa airflow dag code
created: 2024-02-23
tags: ["reference", "migrated", "resource", "airflow"]
PARA: Resource
êµ¬ë¶„: ["Airflow"]
---

# nepa airflow dag code

## ğŸ“ ë‚´ìš©

#airflow-chain #airflow-branch

ì •ë¦¬

- stepë³„ë¡œ ë‚˜ëˆ ì„œ ë””ë ‰í† ë¦¬ ì •ë¦¬

- ë””ë ‰í† ë¦¬ì—ì„œ .py, .sqlì„ ì°¾ì•„ì„œ ì‹¤í–‰

- bash operatorë§Œ ì‚¬ìš©í•´ì„œ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ë˜ì§€, íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ”ì§€ ë‘ ê°œì˜ ê²½ìš°ë¡œë§Œ ì»¨íŠ¸ë¡¤

- ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ì‘ì„±í•œ ìŠ¤í¬ë¦½íŠ¸ ë“±ë“±ì„ ì—°ê²°ì‹œí‚¤ê¸°ë§Œ í•˜ë©´ ë˜ì„œ dagë”°ë¡œ ìŠ¤í¬ë¦½íŠ¸ë”°ë¡œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŒ.

- dag ì‘ì„±í•˜ëŠ” ì‚¬ëŒì€ ìŠ¤í¬ë¦½íŠ¸ ë‹¨ìœ„ë¡œ ëŒì•„ê°€ê²Œ íŒŒì¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”ë¼ê³  ìš”ì²­í•  ìˆ˜ ìˆê³ , ìŠ¤í¬ë¦½íŠ¸ ì§œëŠ” ì‚¬ëŒì€ ìê¸°ê°€ ì§  ì½”ë“œê°€ cliì—ì„œ ëŒì•„ê°€ëŠ”ì§€ë§Œ ì²´í¬í•˜ë©´ë˜ì„œ ì„œë¡œ í¸ë¦¬í•¨

```python
import os
import sys

home_path = "/opt/airflow/scripts"
file_paths = "/opt/airflow/sql/step2"

sys.path.append(home_path)

from airflow_common import *

def decide_branch(**kwargs):

Â  Â  execution_date = kwargs["execution_date"].strftime("%d")

Â  Â  print(execution_date)

Â  Â  if execution_date == "01":

Â  Â  Â  Â  return "trigger_step3_monthly"

Â  Â  else:

Â  Â  Â  Â  return "trigger_step3_daily"

default_args = {

Â  Â  "owner": "airflow",
Â  Â  "retries": 1,
Â  Â  "retry_delay": timedelta(minutes=5),
Â  Â  "catchup": False,
Â  Â  "email_on_failure": True,
Â  Â  "email_on_retry": False,
Â  Â  "email": ["ethan@aivelabs.com"],
}

dag = DAG(
Â  Â  "step2",
Â  Â  default_args=default_args,
Â  Â  start_date=datetime(2024, 2, 12, tzinfo=korean_time_zone),
Â  Â  description="step2 - create master table",
Â  Â  schedule_interval=None,
Â  Â  max_active_runs=1,
Â  Â  concurrency=1,
Â  Â  tags=["step2"],
)

file_paths = sorted(
Â  Â  [
Â  Â  Â  Â  file_name
Â  Â  Â  Â  for file_name in glob(f"{file_paths}/**", recursive=True)
Â  Â  Â  Â  if (".sh" in file_name or ".sql" in file_name)
Â  Â  ]
)

start = DummyOperator(task_id="start", dag=dag)
end = EmptyOperator(task_id="end", trigger_rule="none_failed_min_one_success")

branching_task = BranchPythonOperator(
Â  Â  task_id="branching_task",
Â  Â  python_callable=decide_branch,
Â  Â  provide_context=True,
Â  Â  dag=dag,
)

trigger_step3_monthly = TriggerDagRunOperator(
Â  Â  task_id="trigger_step3_monthly",
Â  Â  trigger_dag_id="step3_monthly",
Â  Â  dag=dag,
)


trigger_step3_daily = TriggerDagRunOperator(
Â  Â  task_id="trigger_step3_daily",
Â  Â  trigger_dag_id="step3_daily",
Â  Â  dag=dag,
)



check_status_all_done = EmptyOperator(
Â  Â  task_id="check_status_all_done", trigger_rule=TriggerRule.ALL_DONE, dag=dag
)

task_list = chain(
Â  Â  *(
Â  Â  Â  Â  [start]
Â  Â  Â  Â  + [
Â  Â  Â  Â  Â  Â  (
Â  Â  Â  Â  Â  Â  Â  Â  PythonOperator(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  task_id=f"{file_path.split('/')[-1][:-4]}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  python_callable=execute_sql_postgres,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  op_kwargs={
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "query_path": f"{file_path}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "execution_date": "{{ execution_date.in_timezone('Asia/Seoul').add(days=0).strftime('%Y%m%d')}}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dag=dag,
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  if ".sql" in file_path
Â  Â  Â  Â  Â  Â  Â  Â  else BashOperator(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  task_id=f"{file_path.split('/')[-1][:-3]}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bash_command=f"bash {file_path} "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  + "{{ execution_date.in_timezone('Asia/Seoul').add(days=0).strftime('%Y%m%d')}}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dag=dag,
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  for file_path in file_paths
Â  Â  Â  Â  ]
Â  Â  Â  Â  + [branching_task]
Â  Â  )
)

branching_task >> trigger_step3_monthly >> check_status_all_done
branching_task >> trigger_step3_daily >> check_status_all_done

check_status_all_done >> end

```

taskflow

#airflow-taskflow

task ì´ë¦„ ë³€ê²½ì€ í…ŒìŠ¤í¬ìƒì„±í•¨ìˆ˜.override(task_id="ë³€ê²½í• íƒœìŠ¤íŠ¸ëª…")

```python
import os

import sys



home_path = "/opt/airflow/scripts"

file_paths = "/opt/airflow/sql/tableau/step0"



sys.path.append(home_path)



from airflow_common import *



tables_config = conf.tables_config_tableau



file_paths = sorted(

Â  Â  [

Â  Â  Â  Â  file_name

Â  Â  Â  Â  for file_name in glob(f"{file_paths}/**", recursive=True)

Â  Â  Â  Â  if (".sh" in file_name or ".sql" in file_name)

Â  Â  ]

)




@dag(

Â  Â  schedule_interval="0 7 1 * *",

Â  Â  start_date=datetime(2024, 4, 15, tzinfo=korean_time_zone),

Â  Â  catchup=False,

Â  Â  # sla_miss_callback=sla_callback,

Â  Â  max_active_runs=1,

Â  Â  concurrency=1,

Â  Â  default_args={"email": "lucas@aivelabs.com"},

Â  Â  tags=["tableau_step0_monthly"],

)

def tableau_step0_monthly():



Â  Â  start = DummyOperator(task_id="start")

Â  Â  end = DummyOperator(task_id="end")



Â  Â  task_list = [start]



Â  Â  # crtbatch -> mktbidb

Â  Â  @task

Â  Â  def execute_query(query_path, destination_table):

Â  Â  Â  Â  source_hook = PostgresHook(postgres_conn_id="crt_bat_stg")

Â  Â  Â  Â  destination_hook = PostgresHook(postgres_conn_id="tableau")



Â  Â  Â  Â  # make tables in crtbatch

Â  Â  Â  Â  source_query = open(query_path, "r").read()

Â  Â  Â  Â  source_records = source_hook.get_records(source_query)



Â  Â  Â  Â  if source_records:

Â  Â  Â  Â  Â  Â  destination_hook.run(sql=f"truncate table {destination_table}")

Â  Â  Â  Â  Â  Â  destination_hook.insert_rows(destination_table, source_records)



Â  Â  for file_path in file_paths:

Â  Â  Â  Â  task_name = f"{file_path.replace('.sql', '')}".split(".")[-1]



Â  Â  Â  Â  if "KPI_1_new_cus_info" in file_path:

Â  Â  Â  Â  Â  Â  destination_table = "aivelabs_sv.new_cus_info"

Â  Â  Â  Â  elif "KPI_2_cus_daily_purchase" in file_path:

Â  Â  Â  Â  Â  Â  destination_table = "aivelabs_sv.cus_daily_purchase"

Â  Â  Â  Â  elif "KPI_3_retention_cal" in file_path:

Â  Â  Â  Â  Â  Â  destination_table = "aivelabs_sv.retention_cal"

Â  Â  Â  Â  elif "cus_bought_monthly" in file_path:

Â  Â  Â  Â  Â  Â  destination_table = "aivelabs_sv.cus_bought_monthly"



Â  Â  Â  Â  globals()[task_name] = execute_query.override(task_id=f"{task_name}")(

Â  Â  Â  Â  Â  Â  file_path, destination_table

Â  Â  Â  Â  )

Â  Â  Â  Â  task_list.append(globals()[task_name])

```

## ğŸ·ï¸ ë¶„ë¥˜

- **PARA**: Resource
- **êµ¬ë¶„**: Airflow

## ğŸ”— ì—°ê²°

**í™œìš© í”„ë¡œì íŠ¸**:
- (ì•„ì§ ì—†ìŒ)

**ê´€ë ¨ ë ˆí¼ëŸ°ìŠ¤**:
- (ì•„ì§ ì—†ìŒ)

---

*Notionì—ì„œ ì¬ë§ˆì´ê·¸ë ˆì´ì…˜ë¨ (2025-11-28)*
