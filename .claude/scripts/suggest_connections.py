import os
import re
import json
from collections import defaultdict
from difflib import SequenceMatcher

vault_base = "/Users/qraft_hongjinyoung/Second-Brain"

print("=" * 100)
print("ğŸ”— ì§€ëŠ¥í˜• ì—°ê²° ì œì•ˆ ì‹œìŠ¤í…œ")
print("=" * 100)

# Load previous analysis
with open('/tmp/connection_analysis.json', 'r', encoding='utf-8') as f:
    analysis = json.load(f)

# Re-scan for detailed analysis
all_notes = {}
for root, dirs, files in os.walk(vault_base):
    if 'automation' in root or '/.git' in root:
        continue
    
    for file in files:
        if not file.endswith('.md'):
            continue
        
        full_path = os.path.join(root, file)
        rel_path = full_path.replace(vault_base + '/', '')
        note_name = file.replace('.md', '')
        
        with open(full_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extract metadata
        title_match = re.search(r'^title:\s*(.+)$', content, re.MULTILINE)
        title = title_match.group(1).strip('"\'') if title_match else note_name
        
        type_match = re.search(r'^type:\s*(\S+)', content, re.MULTILINE)
        doc_type = type_match.group(1) if type_match else 'unknown'
        
        tags_match = re.search(r'tags:\s*\[(.*?)\]', content, re.DOTALL)
        if not tags_match:
            tags_match = re.search(r'tags:\n((?:  - .*\n)*)', content)
        tags = []
        if tags_match:
            tags_str = tags_match.group(1)
            tags = [t.strip().strip('"').replace('  - ', '') for t in re.findall(r'["\']?([^,"\'\n]+)["\']?', tags_str) if t.strip()]
        
        # Extract keywords from content (simple approach)
        content_lower = content.lower()
        keywords = set()
        
        # Common tech terms
        tech_terms = ['airflow', 'dbt', 'snowflake', 'datahub', 'keycloak', 'gitlab', 
                      'postgres', 'python', 'sql', 'docker', 'kubernetes', 'aws']
        for term in tech_terms:
            if term in content_lower:
                keywords.add(term)
        
        all_notes[note_name] = {
            'path': rel_path,
            'full_path': full_path,
            'title': title,
            'type': doc_type,
            'tags': set(tags),
            'keywords': keywords,
            'content': content
        }

# Suggestion algorithm
suggestions = defaultdict(list)

# 1. Tag-based suggestions
print("\nğŸ·ï¸  íƒœê·¸ ê¸°ë°˜ ì—°ê²° ì œì•ˆ")
print("-" * 100)

orphan_paths = set(analysis['orphans'])
orphan_notes = {n: info for n, info in all_notes.items() if info['path'] in orphan_paths}

tag_matches = 0
for orphan_name, orphan_info in list(orphan_notes.items())[:10]:  # Sample first 10
    print(f"\nğŸ“„ {orphan_name}")
    print(f"   Type: {orphan_info['type']} | Tags: {', '.join(orphan_info['tags']) if orphan_info['tags'] else 'None'}")
    
    # Find notes with overlapping tags
    candidates = []
    for note_name, note_info in all_notes.items():
        if note_name == orphan_name:
            continue
        
        common_tags = orphan_info['tags'] & note_info['tags']
        if common_tags:
            candidates.append((note_name, len(common_tags), list(common_tags)))
    
    if candidates:
        candidates.sort(key=lambda x: x[1], reverse=True)
        print(f"   ğŸ’¡ ì œì•ˆ ì—°ê²°:")
        for cand_name, num_common, common_tags in candidates[:3]:
            cand_info = all_notes[cand_name]
            print(f"      â†’ {cand_name} ({cand_info['type']}) - ê³µí†µ íƒœê·¸: {', '.join(common_tags)}")
            suggestions[orphan_name].append({
                'target': cand_name,
                'reason': f'ê³µí†µ íƒœê·¸: {", ".join(common_tags)}',
                'confidence': 'high' if num_common >= 2 else 'medium'
            })
            tag_matches += 1

print(f"\níƒœê·¸ ê¸°ë°˜ ë§¤ì¹­: {tag_matches}ê±´")

# 2. Keyword-based suggestions
print(f"\n\nğŸ”¤ í‚¤ì›Œë“œ ê¸°ë°˜ ì—°ê²° ì œì•ˆ")
print("-" * 100)

keyword_matches = 0
for orphan_name, orphan_info in list(orphan_notes.items())[10:20]:  # Next 10
    if not orphan_info['keywords']:
        continue
    
    print(f"\nğŸ“„ {orphan_name}")
    print(f"   Keywords: {', '.join(orphan_info['keywords'])}")
    
    candidates = []
    for note_name, note_info in all_notes.items():
        if note_name == orphan_name:
            continue
        
        common_keywords = orphan_info['keywords'] & note_info['keywords']
        if common_keywords:
            candidates.append((note_name, len(common_keywords), list(common_keywords)))
    
    if candidates:
        candidates.sort(key=lambda x: x[1], reverse=True)
        print(f"   ğŸ’¡ ì œì•ˆ ì—°ê²°:")
        for cand_name, num_common, common_keywords in candidates[:2]:
            cand_info = all_notes[cand_name]
            print(f"      â†’ {cand_name} ({cand_info['type']}) - í‚¤ì›Œë“œ: {', '.join(common_keywords)}")
            suggestions[orphan_name].append({
                'target': cand_name,
                'reason': f'ê³µí†µ í‚¤ì›Œë“œ: {", ".join(common_keywords)}',
                'confidence': 'medium'
            })
            keyword_matches += 1

print(f"\ní‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­: {keyword_matches}ê±´")

# 3. Type-based suggestions (Experience â†” Project â†” Knowledge)
print(f"\n\nğŸ”º íƒ€ì… ê¸°ë°˜ ì—°ê²° ì œì•ˆ (Experience â†” Project â†” Knowledge)")
print("-" * 100)

# Weekly reflections should link to projects from the same time period
weekly_notes = {n: info for n, info in all_notes.items() if info['type'] == 'weekly-reflection'}
project_notes = {n: info for n, info in all_notes.items() if info['type'] == 'project'}

type_matches = 0
for weekly_name, weekly_info in list(weekly_notes.items())[:5]:
    # Extract date from title (e.g., "2025ë…„ 11ì›” 24ì¼")
    date_match = re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', weekly_info['title'])
    if not date_match:
        continue
    
    year, month, day = date_match.groups()
    week_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
    
    print(f"\nğŸ“… {weekly_name} ({week_date})")
    
    # Find projects from similar time period
    candidates = []
    for proj_name, proj_info in project_notes.items():
        # Extract date from project
        proj_date_match = re.search(r'date:\s*["\']?(\d{4}-\d{2}-\d{2})', proj_info['content'])
        if proj_date_match:
            proj_date = proj_date_match.group(1)
            # Same month?
            if proj_date[:7] == week_date[:7]:
                candidates.append((proj_name, proj_date))
    
    if candidates:
        print(f"   ğŸ’¡ ê°™ì€ ì‹œê¸° í”„ë¡œì íŠ¸:")
        for proj_name, proj_date in candidates[:3]:
            print(f"      â†’ {proj_name} ({proj_date})")
            suggestions[weekly_name].append({
                'target': proj_name,
                'reason': f'ê°™ì€ ì‹œê¸° ({proj_date})',
                'confidence': 'high'
            })
            type_matches += 1

print(f"\níƒ€ì… ê¸°ë°˜ ë§¤ì¹­: {type_matches}ê±´")

# Save suggestions
output = {
    'total_suggestions': sum(len(v) for v in suggestions.values()),
    'notes_with_suggestions': len(suggestions),
    'suggestions': {k: v for k, v in suggestions.items()}
}

with open('/tmp/connection_suggestions.json', 'w', encoding='utf-8') as f:
    json.dump(output, f, ensure_ascii=False, indent=2)

print(f"\n\n" + "=" * 100)
print(f"ğŸ“Š ì œì•ˆ ìš”ì•½")
print("=" * 100)
print(f"ì „ì²´ ì œì•ˆ ìˆ˜: {output['total_suggestions']}ê°œ")
print(f"ì œì•ˆ ë°›ì€ ë…¸íŠ¸: {output['notes_with_suggestions']}ê°œ")
print(f"\nìƒì„¸ ì œì•ˆì´ /tmp/connection_suggestions.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

