#!/usr/bin/env python3
"""
Organize ë³¸ê¹¨ì  files into Life-Insights structure with proper categorization and tagging
"""

import os
import re
import yaml
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict

# Define categorization keywords
WORK_KEYWORDS = [
    'ì—…ë¬´', 'íŒ€', 'í”„ë¡œì íŠ¸', 'íšŒì˜', 'ë°ì´í„°', 'íŒŒì´í”„ë¼ì¸', 'ê³ ê°', 'ë™ë£Œ',
    'ë°ë‹ˆìŠ¤', 'íšŒì‚¬', 'í¬ë˜í”„íŠ¸', 'ë©´ì ‘', 'í˜„ëŒ€', 'ëŒ€í‘œ', 'ë³´ê³ ', 'í˜‘ì—…',
    'ì»¤ë®¤ë‹ˆì¼€ì´ì…˜', 'ì„±ê³¼', 'ì½”ë“œ', 'ê°œë°œ', 'ëŒ€ì‹œë³´ë“œ', 'ì„œë²„', 'í´ë¼ì´ì–¸íŠ¸',
    'ë”œë¦¬ë²„ë¦¬', 'ê¸°íš', 'ì„¤ê³„', 'í…ŒìŠ¤íŠ¸', 'ë°°í¬', 'ì‚¬ìš©í˜„í™©', 'ë¯¸íŒ…',
    'ì˜¤í† ì—ë²„', 'ë‚˜ì´ìŠ¤í‰ê°€ì •ë³´', 'ê³ ìœ„ë“œ', 'í† ìŠ¤', 'ë°€ë¦¬ì˜-ì„œì¬', 'factset',
    'POWER-BI', 'DB', 'ë ˆì•„', 'í´ë¡œì´', 'íŒ¨íŠ¸ë¦­', 'ë§ˆë¹ˆ', 'ì œì´', 'í›ˆ',
    'íŒ€ì›', 'íŒ€ì¥', 'ì‚¬ì¥', 'ì§ì›', 'ê²½ìŸë ¥', 'ê²½ë ¥', 'ì±„ìš©', 'í•©ê²©', 'ì½”í…Œ'
]

PERSONAL_KEYWORDS = [
    'ê·œë¦¼', 'ê·œë¦¬ë¯¸', 'ê°€ì¡±', 'ì—„ë§ˆ', 'ì•„ë¹ ', 'ì¹œêµ¬', 'ê²°í˜¼', 'ìš´ë™', 'ì—¬í–‰',
    'ì·¨ë¯¸', 'ì½˜ì„œíŠ¸', 'ë§ˆë¼í†¤', 'í—¬ìŠ¤', 'ë¡¯ë°ì›”ë“œ', 'ë¶ˆê½ƒì¶•ì œ', 'ë°±ì˜ˆë¦°',
    'ì½œë“œí”Œë ˆì´', 'í•œë¼ì‚°', 'ì•„ì¹´ë°ë¯¸í•˜ìš°ìŠ¤', 'ì•„ì´í°', 'ì•„ìš¸ë ›', 'êµìœ¡ë¹„',
    'ë¯¼ì² ', 'ì„¸í›ˆ', 'ì…˜', 'ê³ ëª¨', 'ì™¸í• ë¨¸ë‹ˆ', 'ìƒê²¬ë¡€', 'ì´ì‚¬', 'ì§‘',
    'ë„ëª¨ì¹´ë ˆ', 'ë³¼íŠ¸', 'ë…¸ëŸ‰ì§„ì§‘', 'ì¶œí‡´ê·¼', 'ê±´ê°•', 'ê°„í˜¸', 'í˜•'
]

WORK_TAGS_MAP = {
    'team': ['íŒ€', 'íŒ€ì›', 'íŒ€ì¥', 'ë™ë£Œ', 'ë ˆì•„', 'í´ë¡œì´', 'íŒ¨íŠ¸ë¦­', 'ë§ˆë¹ˆ'],
    'project': ['í”„ë¡œì íŠ¸', 'ê¸°íš', 'ì„¤ê³„', 'ê°œë°œ', 'ë°°í¬'],
    'communication': ['íšŒì˜', 'ë¯¸íŒ…', 'ë³´ê³ ', 'ì†Œí†µ', 'ì»¤ë®¤ë‹ˆì¼€ì´ì…˜', 'í˜‘ì—…'],
    'growth': ['ì„±ì¥', 'í•™ìŠµ', 'ê³µë¶€', 'ë°°ì›€', 'ê¹¨ë‹¬ìŒ'],
    'learning': ['ë°°ì›€', 'ê³µë¶€', 'í›ˆë ¨', 'ì—°ìŠµ', 'í•™ìŠµ'],
    'frustration': ['ë‹µë‹µ', 'ì—­ê²¨ìš´', 'ë¶ˆë§Œ', 'í™”', 'ì‹œë¶ˆ', 'í˜ë“¤'],
    'achievement': ['ì„±ê³¼', 'ì¸ì •', 'ì„±ê³µ', 'ì˜í•¨', 'ê°œì„ '],
    'process': ['ê³„íš', 'ì§„í–‰', 'ê´€ë¦¬', 'ì •ë¦¬', 'ì„¤ê³„', 'êµ¬ì¡°'],
    'conflict': ['ë¶ˆí™”', 'ê¸°ë¥˜', 'ì–´ë ¤ì›€', 'ë¬¸ì œ', 'ì˜¤ë¥˜'],
    'collaboration': ['í˜‘ì—…', 'ê°™ì´', 'í•¨ê»˜', 'ê³µìœ '],
    'career': ['ë©´ì ‘', 'ì±„ìš©', 'ì´ì§', 'í•©ê²©', 'ì—°ë´‰'],
    'leadership': ['ì‚¬ì¥', 'ëŒ€í‘œ', 'CEO', 'CFO', 'MD'],
    'technical': ['ì½”ë“œ', 'ë°ì´í„°', 'ì„œë²„', 'DB', 'íŒŒì´í”„ë¼ì¸', 'ëŒ€ì‹œë³´ë“œ', 'BI']
}

PERSONAL_TAGS_MAP = {
    'relationship': ['ê·œë¦¼', 'ì‚¬ë‘', 'ì—°ì• ', 'ë°ì´íŠ¸', 'ê´€ê³„'],
    'family': ['ê°€ì¡±', 'ì—„ë§ˆ', 'ì•„ë¹ ', 'ë¶€ëª¨', 'ì¹œì²™', 'ì™¸í• ë¨¸ë‹ˆ'],
    'reflection': ['ìƒê°', 'ê¹¨ë‹¬ìŒ', 'ëŠë‚€', 'ë°˜ì„±'],
    'love': ['ì‚¬ë‘', 'ê²°í˜¼', 'í—¤ì–´', 'ê·œë¦¼'],
    'health': ['ìš´ë™', 'í—¬ìŠ¤', 'ë§ˆë¼í†¤', 'ê±´ê°•', 'ê°„í˜¸'],
    'friends': ['ì¹œêµ¬', 'ë¯¼ì² ', 'ì„¸í›ˆ', 'ì…˜', 'ë¡œëª¬'],
    'life-decision': ['ê²°ì‹¬', 'ì„ íƒ', 'ë„ì „', 'ë³€í™”'],
    'gratitude': ['ê°ì‚¬', 'ê³ ë§ˆìš´', 'ë°°í‘¸'],
    'happiness': ['í–‰ë³µ', 'ì¦ê±°ì›€', 'ê¸ì •', 'ì›ƒìŒ'],
    'entertainment': ['ì½˜ì„œíŠ¸', 'ì¶•ì œ', 'ì—¬í–‰', 'ì·¨ë¯¸']
}

OBSERVATION_TAGS_MAP = {
    'philosophy': ['ì¸ìƒ', 'ì² í•™', 'ì˜ë¯¸', 'ê°€ì¹˜'],
    'insight': ['ê¹¨ë‹¬ìŒ', 'ë°°ìš´', 'ëŠë‚€'],
    'human-nature': ['ì‚¬ëŒ', 'ì¸ê°„', 'ì¸í’ˆ', 'ì„±ê²©'],
    'work-life': ['ì¼', 'ì—…ë¬´', 'ì§ì¥', 'íšŒì‚¬'],
    'mindset': ['ë§ˆì¸ë“œ', 'ìƒê°', 'íƒœë„', 'ìì„¸']
}

def extract_frontmatter(content: str) -> Tuple[Dict, str]:
    """Extract YAML frontmatter from markdown content"""
    if content.startswith('---'):
        parts = content.split('---', 2)
        if len(parts) >= 3:
            try:
                frontmatter = yaml.safe_load(parts[1])
                body = parts[2].strip()
                return frontmatter or {}, body
            except:
                pass
    return {}, content

def categorize_file(title: str, frontmatter: Dict, body: str) -> str:
    """Categorize file into Work/Personal/Observations"""
    text = f"{title} {frontmatter.get('ê¹¨ë‹¬ì€ ê²ƒ', '')} {frontmatter.get('ë³¸ ê²ƒ', '')} {frontmatter.get('ì¼ê¸°', '')} {body}".lower()

    work_score = sum(1 for kw in WORK_KEYWORDS if kw.lower() in text)
    personal_score = sum(1 for kw in PERSONAL_KEYWORDS if kw.lower() in text)

    # Strong work indicators
    if any(kw in title for kw in ['ë©´ì ‘', 'í•©ê²©', 'ì½”í…Œ', 'ë¯¸íŒ…', 'ë³´ê³ ', 'íŒ€', 'í”„ë¡œì íŠ¸']):
        return 'Work'

    # Strong personal indicators
    if any(kw in title for kw in ['ê·œë¦¼', 'ê°€ì¡±', 'ì—„ë§ˆ', 'ì•„ë¹ ', 'ì¹œêµ¬', 'ê²°í˜¼', 'ì½˜ì„œíŠ¸', 'ì—¬í–‰']):
        return 'Personal'

    # Check íšŒê³ ì¢…ë¥˜
    review_type = frontmatter.get('íšŒê³ ì¢…ë¥˜', '')
    if 'ì£¼ê°„íšŒê³ ' in review_type or 'ì¼ì¼íšŒê³ ' in review_type:
        if work_score > personal_score:
            return 'Work'
        elif personal_score > work_score:
            return 'Personal'

    # Score-based decision
    if work_score > personal_score * 1.5:
        return 'Work'
    elif personal_score > work_score * 1.5:
        return 'Personal'
    elif work_score > 0 or personal_score > 0:
        # Has some specific keywords but unclear
        if work_score >= personal_score:
            return 'Work'
        else:
            return 'Personal'

    # Default to observations for philosophical/general content
    return 'Observations'

def extract_tags(title: str, frontmatter: Dict, body: str, category: str) -> List[str]:
    """Extract relevant tags based on content and category"""
    text = f"{title} {frontmatter.get('ì ìš©í•  ê²ƒ', '')} {frontmatter.get('ê¹¨ë‹¬ì€ ê²ƒ', '')} {frontmatter.get('ë³¸ ê²ƒ', '')} {frontmatter.get('ì¼ê¸°', '')} {body}".lower()

    tags = set()

    # Add category tag
    tags.add(category.lower())

    # Add based on category
    if category == 'Work':
        for tag, keywords in WORK_TAGS_MAP.items():
            if any(kw.lower() in text for kw in keywords):
                tags.add(tag)
    elif category == 'Personal':
        for tag, keywords in PERSONAL_TAGS_MAP.items():
            if any(kw.lower() in text for kw in keywords):
                tags.add(tag)
    else:  # Observations
        for tag, keywords in OBSERVATION_TAGS_MAP.items():
            if any(kw.lower() in text for kw in keywords):
                tags.add(tag)

    # Add review type
    review_type = frontmatter.get('íšŒê³ ì¢…ë¥˜', '')
    if review_type:
        tags.add('reflection')

    return sorted(list(tags))

def find_similar_files(file_info: Dict, all_files: List[Dict], max_related: int = 5) -> List[str]:
    """Find similar files based on tags and content"""
    current_tags = set(file_info['tags'])
    similar = []

    for other in all_files:
        if other['path'] == file_info['path']:
            continue

        other_tags = set(other['tags'])
        overlap = len(current_tags & other_tags)

        if overlap >= 2:  # At least 2 common tags
            similar.append((overlap, other['title'], other['new_path']))

    # Sort by overlap and return top matches
    similar.sort(reverse=True, key=lambda x: x[0])
    return [f"[[{path}|{title}]]" for _, title, path in similar[:max_related]]

def process_files():
    """Main processing function"""
    source_dir = Path('/Users/qraft_hongjinyoung/DAE-Second-Brain/ë³¸ê¹¨ì ')
    target_base = Path('/Users/qraft_hongjinyoung/DAE-Second-Brain/30-Flow/Life-Insights')

    # Collect all file info first
    all_files = []
    category_counts = defaultdict(int)
    tag_counts = defaultdict(int)

    print("ğŸ“Š Reading all files...")
    for md_file in source_dir.glob('*.md'):
        content = md_file.read_text(encoding='utf-8')
        frontmatter, body = extract_frontmatter(content)

        title = frontmatter.get('title', md_file.stem)
        category = categorize_file(title, frontmatter, body)
        tags = extract_tags(title, frontmatter, body, category)

        new_path = f"30-Flow/Life-Insights/{category}/{md_file.name}"

        all_files.append({
            'path': str(md_file),
            'title': title,
            'category': category,
            'tags': tags,
            'frontmatter': frontmatter,
            'body': body,
            'new_path': new_path,
            'filename': md_file.name
        })

        category_counts[category] += 1
        for tag in tags:
            tag_counts[tag] += 1

    print(f"âœ… Analyzed {len(all_files)} files\n")

    # Print summary
    print("ğŸ“ Category Distribution:")
    for cat, count in sorted(category_counts.items()):
        print(f"  - {cat}: {count} files")

    print(f"\nğŸ·ï¸  Top 20 Tags:")
    for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:20]:
        print(f"  - {tag}: {count}")

    # Process each file
    print(f"\nğŸ”„ Moving and updating files...")
    moved_count = 0
    error_count = 0

    for file_info in all_files:
        try:
            # Find related files
            related = find_similar_files(file_info, all_files)

            # Update frontmatter
            fm = file_info['frontmatter']

            # Add/update tags
            existing_tags = fm.get('tags', [])
            if not isinstance(existing_tags, list):
                existing_tags = [existing_tags] if existing_tags else []

            # Merge tags (keep notion-import, ë³¸ê¹¨ì , add new ones)
            new_tags = list(set(existing_tags + file_info['tags']))
            fm['tags'] = new_tags

            # Build new content
            new_content = "---\n"
            new_content += yaml.dump(fm, allow_unicode=True, default_flow_style=False)
            new_content += "---\n\n"
            new_content += file_info['body']

            # Add related section if we have related files
            if related and '## Related' not in file_info['body']:
                new_content += "\n\n## Related\n\n"
                new_content += "\n".join(f"- {link}" for link in related)
                new_content += "\n"

            # Write to new location
            target_path = target_base / file_info['category'] / file_info['filename']
            target_path.parent.mkdir(parents=True, exist_ok=True)
            target_path.write_text(new_content, encoding='utf-8')

            moved_count += 1

            if moved_count % 50 == 0:
                print(f"  Processed {moved_count}/{len(all_files)} files...")

        except Exception as e:
            print(f"  âŒ Error processing {file_info['filename']}: {e}")
            error_count += 1

    print(f"\nâœ… Successfully moved {moved_count} files")
    if error_count > 0:
        print(f"âŒ Encountered {error_count} errors")

    # Final summary
    print(f"\nğŸ“Š Final Summary:")
    print(f"  Total files: {len(all_files)}")
    print(f"  Work: {category_counts['Work']}")
    print(f"  Personal: {category_counts['Personal']}")
    print(f"  Observations: {category_counts['Observations']}")
    print(f"  Unique tags: {len(tag_counts)}")
    print(f"  Cross-references created: {sum(1 for f in all_files if find_similar_files(f, all_files))}")

if __name__ == '__main__':
    process_files()
