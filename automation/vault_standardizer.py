#!/usr/bin/env python3
"""
Vault Standardization Script
============================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Obsidian vaultì˜ frontmatter, íƒœê·¸, ì—°ê²°ì„±ì„ í‘œì¤€í™”í•©ë‹ˆë‹¤.

Usage:
    python vault_standardizer.py --phase 1 --dry-run
    python vault_standardizer.py --phase 2 --area "30-Flow/Life-Insights/Personal"
    python vault_standardizer.py --all --apply

Phases:
    1. Type í‘œì¤€í™”
    2. íƒœê·¸ í‘œì¤€í™”
    3. í•„ìˆ˜ í•„ë“œ ì¶”ê°€
    4. Related ì„¹ì…˜ ìƒì„±
    5. ë°±ë§í¬ ê°•í™”
"""

import os
import re
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set
from dataclasses import dataclass
import argparse


@dataclass
class Note:
    """ë…¸íŠ¸ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    path: Path
    frontmatter: Dict
    content: str
    has_frontmatter: bool = True


class VaultStandardizer:
    """Vault í‘œì¤€í™” í´ë˜ìŠ¤"""

    # Type ë³€í™˜ ë§¤í•‘
    TYPE_MAPPING = {
        "ì£¼ê°„íšŒê³ ": "weekly-reflection",
        "daily-insight": "insight",
        "daily-reflection": "reflection",
        "ì¼ì¼íšŒê³ ": "reflection",
        "í•˜ë£¨ì¼ê¸°": "insight",
    }

    # íƒœê·¸ ë³€í™˜ ë§¤í•‘ (í•œê¸€ â†’ ì˜ì–´)
    TAG_MAPPING = {
        "ì»¤ë¦¬ì–´": "career",
        "ì»¤ë¦¬ì–´-ì§€ì›ë‚´ì—­": "career-application",
        "ë¬¸ì œí•´ê²°": "problem-solving",
        "ë°ì´í„°ê±°ë²„ë„ŒìŠ¤": "data-governance",
        "êµ¬ì¡°í™”": "structuring",
        "ë¬¸ì„œí™”": "documentation",
        "ì˜ì‚¬ì†Œí†µ": "communication",
        "í˜‘ì—…": "collaboration",
        "ê¸°ìˆ ì „íŒŒ": "knowledge-sharing",
        "ë¹„ìš© ìµœì í™”": "cost-optimization",
        "ì„±ëŠ¥ê°œì„ ": "performance-optimization",
        "ìš´ì˜ ì²´ê³„í™”": "operation-systematization",
        "ìë™í™”": "automation",
        "ì£¼ì‹íˆ¬ì": "stock-investment",
        "íˆ¬ìë…¸íŠ¸": "investment-note",
        "ì´ì§": "job-change",
        "í˜„ëŒ€ì˜¤í† ì—ë²„": "hyundai-autoever",
        "ì„±ê³¼ì§ˆë¬¸": "achievement-question",
        "ì•Œê³ ë¦¬ì¦˜": "algorithm",
        "ì¸ê°„ê´€ê³„": "relationship",
        "ê°€ì¡±": "family",
        "ì¹œêµ¬": "friends",
        "ì—°ì• ": "love",
        "ì¸ìƒê²°ì •": "life-decision",
        "ì„±ì°°": "reflection",
        "ê´€ì°°": "observations",
        "ê°œì¸": "personal",
        "ì¼ìƒ": "daily",
        "ì—…ë¬´": "work",
        "íšŒì‚¬ìƒí™œ": "work-life",
        "ì‚¬ê³ ë°©ì‹": "mindset",
        "ì² í•™": "philosophy",
        "ì¸ê°„ë³¸ì„±": "human-nature",
    }

    # í‘œì¤€ íƒ€ì… ëª©ë¡
    STANDARD_TYPES = {
        "resource", "project", "reflection", "insight", "map", "moc",
        "weekly-reflection", "permanent", "literature", "fleeting",
        "experience", "outcome", "knowledge", "guide", "documentation"
    }

    def __init__(self, vault_path: str, dry_run: bool = True):
        self.vault_path = Path(vault_path)
        self.dry_run = dry_run
        self.stats = {
            "total_files": 0,
            "processed": 0,
            "skipped": 0,
            "errors": 0,
            "changes": []
        }

    def find_markdown_files(self, area: Optional[str] = None) -> List[Path]:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì°¾ê¸°"""
        search_path = self.vault_path / area if area else self.vault_path

        # .git, automation ë“± ì œì™¸
        exclude_dirs = {".git", "automation", "node_modules", ".obsidian"}

        files = []
        for file_path in search_path.rglob("*.md"):
            # ì œì™¸ ë””ë ‰í† ë¦¬ ì²´í¬
            if any(ex in file_path.parts for ex in exclude_dirs):
                continue
            files.append(file_path)

        return files

    def parse_note(self, file_path: Path) -> Note:
        """ë…¸íŠ¸ íŒŒì‹±"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Frontmatter ì¶”ì¶œ
            fm_match = re.match(r'^---\n(.*?)\n---\n(.*)$', content, re.DOTALL)

            if fm_match:
                fm_text = fm_match.group(1)
                body = fm_match.group(2)

                try:
                    frontmatter = yaml.safe_load(fm_text) or {}
                except yaml.YAMLError:
                    frontmatter = {}

                return Note(
                    path=file_path,
                    frontmatter=frontmatter,
                    content=body,
                    has_frontmatter=True
                )
            else:
                return Note(
                    path=file_path,
                    frontmatter={},
                    content=content,
                    has_frontmatter=False
                )

        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path} - {e}")
            return None

    def standardize_type(self, note: Note) -> Dict:
        """Type í•„ë“œ í‘œì¤€í™”"""
        changes = []
        fm = note.frontmatter.copy()

        if "type" in fm:
            old_type = fm["type"]

            # ë¬¸ìì—´ ì •ë¦¬ (ë”°ì˜´í‘œ ì œê±° ë“±)
            if isinstance(old_type, str):
                old_type = old_type.strip().strip('"').strip("'")

                # ë§¤í•‘ ì ìš©
                if old_type in self.TYPE_MAPPING:
                    new_type = self.TYPE_MAPPING[old_type]
                    fm["type"] = new_type
                    changes.append(f"Type: '{old_type}' â†’ '{new_type}'")

                # ë¹„í‘œì¤€ íƒ€ì… ê²½ê³ 
                elif old_type not in self.STANDARD_TYPES:
                    changes.append(f"âš ï¸  ë¹„í‘œì¤€ íƒ€ì…: '{old_type}'")
        else:
            # Type í•„ë“œ ì—†ìœ¼ë©´ ìë™ ì¶”ì •
            inferred_type = self._infer_type(note)
            if inferred_type:
                fm["type"] = inferred_type
                changes.append(f"Type ì¶”ê°€: '{inferred_type}'")

        return fm, changes

    def standardize_tags(self, note: Note) -> Dict:
        """íƒœê·¸ í‘œì¤€í™”"""
        changes = []
        fm = note.frontmatter.copy()

        if "tags" not in fm:
            fm["tags"] = []
            changes.append("Tags í•„ë“œ ì¶”ê°€")

        tags = fm["tags"]

        # ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(tags, str):
            # í•´ì‹œíƒœê·¸ í˜•ì‹ ì²˜ë¦¬
            if tags.startswith("#"):
                tags = [t.strip() for t in tags.split() if t.startswith("#")]
                tags = [t[1:] for t in tags]  # # ì œê±°
            else:
                tags = [tags]

        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ë‚´ìš© ê¸°ë°˜ íƒœê·¸ ì¶”ì •
        if not tags or tags == []:
            inferred_tags = self._infer_tags(note)
            if inferred_tags:
                tags = inferred_tags
                changes.append(f"Tags ìë™ ì¶”ê°€: {inferred_tags}")

        # íƒœê·¸ ë³€í™˜ (í•œê¸€ â†’ ì˜ì–´)
        new_tags = []
        for tag in tags:
            if isinstance(tag, str):
                tag = tag.strip()

                # í•œê¸€ íƒœê·¸ ë³€í™˜
                if tag in self.TAG_MAPPING:
                    new_tag = self.TAG_MAPPING[tag]
                    new_tags.append(new_tag)
                    if tag != new_tag:
                        changes.append(f"Tag: '{tag}' â†’ '{new_tag}'")
                else:
                    # ì†Œë¬¸ìí™” ë° ì •ë¦¬
                    new_tag = tag.lower().replace("_", "-").replace(" ", "-")
                    new_tags.append(new_tag)
                    if tag != new_tag:
                        changes.append(f"Tag ì •ë¦¬: '{tag}' â†’ '{new_tag}'")

        fm["tags"] = list(set(new_tags))  # ì¤‘ë³µ ì œê±°

        return fm, changes

    def add_missing_fields(self, note: Note) -> Dict:
        """ëˆ„ë½ëœ í•„ë“œ ì¶”ê°€"""
        changes = []
        fm = note.frontmatter.copy()

        # created í•„ë“œ
        if "created" not in fm:
            # íŒŒì¼ ìƒì„±ì¼ ì‚¬ìš©
            created = datetime.fromtimestamp(note.path.stat().st_ctime)
            fm["created"] = created.strftime("%Y-%m-%d")
            changes.append(f"Created ì¶”ê°€: {fm['created']}")

        # updated í•„ë“œ
        if "updated" not in fm:
            # íŒŒì¼ ìˆ˜ì •ì¼ ì‚¬ìš©
            updated = datetime.fromtimestamp(note.path.stat().st_mtime)
            fm["updated"] = updated.strftime("%Y-%m-%d")
            changes.append(f"Updated ì¶”ê°€: {fm['updated']}")

        # title í•„ë“œ (ì—†ìœ¼ë©´ íŒŒì¼ëª… ì‚¬ìš©)
        if "title" not in fm:
            title = note.path.stem.replace("-", " ")
            fm["title"] = title
            changes.append(f"Title ì¶”ê°€: '{title}'")

        # aliases í•„ë“œ
        if "aliases" not in fm:
            fm["aliases"] = []

        return fm, changes

    def create_related_section(self, note: Note) -> str:
        """Related ì„¹ì…˜ ìƒì„±"""
        # ê¸°ì¡´ Related ì„¹ì…˜ í™•ì¸
        if "## Related" in note.content or "## ğŸ“ Related" in note.content:
            return note.content, []

        # íƒœê·¸ ê¸°ë°˜ ê´€ë ¨ ë…¸íŠ¸ ì°¾ê¸° (ê°„ë‹¨í•œ ë²„ì „)
        related_section = "\n---\n\n## ğŸ“ Related\n\n"
        related_section += "<!-- ìë™ ìƒì„±ëœ ì„¹ì…˜ - ìˆ˜ë™ìœ¼ë¡œ ë§í¬ë¥¼ ì¶”ê°€í•˜ì„¸ìš” -->\n\n"
        related_section += "### Projects\n\n"
        related_section += "### Knowledge\n\n"
        related_section += "### Insights\n\n"

        new_content = note.content.rstrip() + "\n" + related_section

        return new_content, ["Related ì„¹ì…˜ ì¶”ê°€"]

    def write_note(self, note: Note, new_fm: Dict, new_content: str):
        """ë…¸íŠ¸ ì €ì¥"""
        # Frontmatterë¥¼ YAMLë¡œ ë³€í™˜
        fm_yaml = yaml.dump(new_fm, allow_unicode=True, sort_keys=False)

        # ì „ì²´ ë‚´ìš© ì¡°í•©
        full_content = f"---\n{fm_yaml}---\n{new_content}"

        if self.dry_run:
            print(f"  [DRY RUN] ì €ì¥í•  ë‚´ìš©:")
            print(f"  {full_content[:200]}...")
        else:
            try:
                with open(note.path, 'w', encoding='utf-8') as f:
                    f.write(full_content)
                print(f"  âœ… ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
                self.stats["errors"] += 1

    def _infer_type(self, note: Note) -> Optional[str]:
        """ë…¸íŠ¸ íƒ€ì… ìë™ ì¶”ì •"""
        path_str = str(note.path)

        if "Projects/" in path_str:
            return "project"
        elif "Weekly/" in path_str:
            return "weekly-reflection"
        elif "Life-Insights/" in path_str:
            return "insight"
        elif "Resources/" in path_str:
            return "resource"
        elif "Zettelkasten/Permanent" in path_str:
            return "permanent"
        elif "Zettelkasten/Literature" in path_str:
            return "literature"
        elif "Zettelkasten/Fleeting" in path_str:
            return "fleeting"

        return None

    def _infer_tags(self, note: Note) -> List[str]:
        """ë…¸íŠ¸ íƒœê·¸ ìë™ ì¶”ì • (í–¥ìƒëœ ë²„ì „)"""
        tags = []
        path_str = str(note.path)
        content = note.content
        content_lower = content.lower()

        # 1. ê²½ë¡œ ê¸°ë°˜ íƒœê·¸
        if "í¬ë˜í”„íŠ¸í…Œí¬ë†€ë¡œì§€ìŠ¤" in path_str or "qraft" in content_lower:
            tags.append("qraft")
        if "Career/" in path_str:
            tags.append("career")
        if "Technology/" in path_str:
            tags.append("technology")
        if "Life-Insights/Work" in path_str:
            tags.append("work")
        if "Life-Insights/Personal" in path_str:
            tags.append("personal")
        if "Observations" in path_str:
            tags.append("observations")

        # 2. ê¸°ìˆ  í‚¤ì›Œë“œ (í™•ì¥)
        tech_keywords = {
            "airflow": "airflow",
            "dbt": "dbt",
            "python": "python",
            "sql": "sql",
            "aws": "aws",
            "docker": "docker",
            "kubernetes": "kubernetes",
            "jenkins": "jenkins",
            "kafka": "kafka",
            "snowflake": "snowflake",
            "postgres": "postgres",
            "datahub": "datahub",
            "ë°ì´í„°": "data",
            "íŒŒì´í”„ë¼ì¸": "pipeline",
            "ì¿¼ë¦¬": "query",
        }

        for keyword, tag in tech_keywords.items():
            if keyword in content_lower:
                tags.append(tag)

        # 3. ê°ì •/ì£¼ì œ í‚¤ì›Œë“œ
        emotion_keywords = {
            "ìŠ¤íŠ¸ë ˆìŠ¤": "stress",
            "í™”": "anger",
            "ë¶„ë…¸": "anger",
            "ì–µìš¸": "frustration",
            "ë‹µë‹µ": "frustration",
            "í–‰ë³µ": "happiness",
            "ê¸°ì¨": "joy",
            "ìŠ¬í””": "sadness",
            "ìš°ìš¸": "depression",
            "ë¶ˆì•ˆ": "anxiety",
            "ê±±ì •": "worry",
        }

        for keyword, tag in emotion_keywords.items():
            if keyword in content:
                tags.append(tag)

        # 4. ì—…ë¬´ ê´€ë ¨ í‚¤ì›Œë“œ
        work_keywords = {
            "íšŒì‚¬": "company",
            "ì—…ë¬´": "work",
            "í”„ë¡œì íŠ¸": "project",
            "íŒ€": "team",
            "ë¯¸íŒ…": "meeting",
            "íšŒì˜": "meeting",
            "ìƒì‚¬": "boss",
            "ë™ë£Œ": "colleague",
            "ì¶œê·¼": "commute",
            "ì•¼ê·¼": "overtime",
            "ì„±ê³¼": "achievement",
            "ëª©í‘œ": "goal",
        }

        for keyword, tag in work_keywords.items():
            if keyword in content:
                tags.append(tag)

        # 5. ì¸ê°„ê´€ê³„ í‚¤ì›Œë“œ
        relationship_keywords = {
            "ê°€ì¡±": "family",
            "ë¶€ëª¨": "family",
            "ì—„ë§ˆ": "family",
            "ì•„ë¹ ": "family",
            "ì¹œêµ¬": "friends",
            "ì—°ì• ": "love",
            "ì• ì¸": "love",
            "ê²°í˜¼": "marriage",
            "ì‚¬ëŒ": "relationships",
        }

        for keyword, tag in relationship_keywords.items():
            if keyword in content:
                tags.append(tag)

        # 6. ìê¸°ê³„ë°œ í‚¤ì›Œë“œ
        growth_keywords = {
            "ë°°ì›€": "learning",
            "ê³µë¶€": "study",
            "ì„±ì¥": "growth",
            "ë°œì „": "development",
            "ê°œì„ ": "improvement",
            "ëª©í‘œ": "goal",
            "ê³„íš": "planning",
            "ì‹œê°„ê´€ë¦¬": "time-management",
        }

        for keyword, tag in growth_keywords.items():
            if keyword in content:
                tags.append(tag)

        # 7. ì¸ì‚¬ì´íŠ¸ íƒ€ì… ì¶”ì •
        if "## ë³¸ ê²ƒ" in content or "## ê¹¨ë‹¬ì€ ê²ƒ" in content:
            tags.append("reflection")
        if "## ì¼ê¸°" in content:
            tags.append("journal")
        if "## ì ìš©í•  ê²ƒ" in content:
            tags.append("action-item")

        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 8ê°œë§Œ ë°˜í™˜
        return list(set(tags))[:8]

    def process_phase(self, phase: int, area: Optional[str] = None):
        """íŠ¹ì • Phase ì‹¤í–‰"""
        files = self.find_markdown_files(area)
        self.stats["total_files"] = len(files)

        print(f"\n{'='*60}")
        print(f"Phase {phase} ì‹¤í–‰: {len(files)}ê°œ íŒŒì¼")
        print(f"ì˜ì—­: {area or 'ì „ì²´'}")
        print(f"ëª¨ë“œ: {'DRY RUN (í…ŒìŠ¤íŠ¸)' if self.dry_run else 'APPLY (ì‹¤ì œ ì ìš©)'}")
        print(f"{'='*60}\n")

        for file_path in files:
            note = self.parse_note(file_path)
            if not note:
                self.stats["skipped"] += 1
                continue

            print(f"\nğŸ“„ {file_path.relative_to(self.vault_path)}")

            changes = []
            new_fm = note.frontmatter.copy()
            new_content = note.content

            # Phaseë³„ ì²˜ë¦¬
            if phase == 1:  # Type í‘œì¤€í™”
                new_fm, type_changes = self.standardize_type(note)
                changes.extend(type_changes)

            elif phase == 2:  # íƒœê·¸ í‘œì¤€í™”
                new_fm, tag_changes = self.standardize_tags(note)
                changes.extend(tag_changes)

            elif phase == 3:  # í•„ìˆ˜ í•„ë“œ ì¶”ê°€
                new_fm, field_changes = self.add_missing_fields(note)
                changes.extend(field_changes)

            elif phase == 4:  # Related ì„¹ì…˜
                new_content, related_changes = self.create_related_section(note)
                changes.extend(related_changes)

            # ë³€ê²½ì‚¬í•­ ì¶œë ¥
            if changes:
                for change in changes:
                    print(f"  â€¢ {change}")

                # ì €ì¥
                self.write_note(note, new_fm, new_content)
                self.stats["processed"] += 1
                self.stats["changes"].extend(changes)
            else:
                print(f"  â„¹ï¸  ë³€ê²½ì‚¬í•­ ì—†ìŒ")
                self.stats["skipped"] += 1

        # í†µê³„ ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"ì™„ë£Œ!")
        print(f"  ì´ íŒŒì¼: {self.stats['total_files']}")
        print(f"  ì²˜ë¦¬ë¨: {self.stats['processed']}")
        print(f"  ê±´ë„ˆëœ€: {self.stats['skipped']}")
        print(f"  ì˜¤ë¥˜: {self.stats['errors']}")
        print(f"  ì´ ë³€ê²½: {len(self.stats['changes'])}")
        print(f"{'='*60}\n")


def main():
    parser = argparse.ArgumentParser(description="Vault í‘œì¤€í™” ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--phase", type=int, choices=[1, 2, 3, 4, 5],
                       help="ì‹¤í–‰í•  Phase (1=Type, 2=Tags, 3=Fields, 4=Related, 5=Backlinks)")
    parser.add_argument("--all", action="store_true",
                       help="ëª¨ë“  Phase ì‹¤í–‰")
    parser.add_argument("--area", type=str,
                       help="íŠ¹ì • ì˜ì—­ë§Œ ì²˜ë¦¬ (ì˜ˆ: '30-Flow/Life-Insights/Personal')")
    parser.add_argument("--dry-run", action="store_true", default=True,
                       help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê¸°ë³¸ê°’)")
    parser.add_argument("--apply", action="store_true",
                       help="ì‹¤ì œ ì ìš© (--dry-run ë¹„í™œì„±í™”)")

    args = parser.parse_args()

    # Vault ê²½ë¡œ
    vault_path = Path(__file__).parent.parent

    # Dry run ì„¤ì •
    dry_run = not args.apply

    # Standardizer ìƒì„±
    standardizer = VaultStandardizer(vault_path, dry_run=dry_run)

    # Phase ì‹¤í–‰
    if args.all:
        for phase in [1, 2, 3, 4]:
            standardizer.process_phase(phase, args.area)
    elif args.phase:
        standardizer.process_phase(args.phase, args.area)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
